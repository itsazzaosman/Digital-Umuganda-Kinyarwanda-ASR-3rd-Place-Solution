{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "os.environ[\"NEMO_CACHE_DIR\"] = \"/shared/A_track/\"\n",
    "glob.glob(\"/shared/KASR/nemo/nemo_experiments/default/checkpoints/epoch=*.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354a8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /shared/KASR/nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# import KASR.nemo.utils as utils\n",
    "device = utils.get_device_safe_threading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3632cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "epoch = \"14\"\n",
    "checkpoint_paths = glob.glob(f\"/shared/KASR/nemo/nemo_experiments/default/checkpoints/epoch={epoch}*.ckpt\")\n",
    "checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3922892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import EncDecCTCModelBPE\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = checkpoint_paths[-1]\n",
    "\n",
    "# Path to save the exported .nemo file\n",
    "nemo_path = f\"/shared/KASR/nemo/nemo_experiments/default/finetuned_epoch={epoch}_model.nemo\"\n",
    "\n",
    "# Load model from .ckpt\n",
    "asr_model = EncDecCTCModelBPE.load_from_checkpoint(checkpoint_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "asr_model.decoding.strategy = 'greedy_batch'\n",
    "\n",
    "# Export to .nemo file\n",
    "asr_model.save_to(nemo_path)\n",
    "\n",
    "print(f\"Model exported to {nemo_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"/shared/track_a_audio_files\"\n",
    "train_json_path = \"/shared/A_track/test.json\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_df = pd.read_json(train_json_path).T\n",
    "\n",
    "\n",
    "test_df[\"file_path\"] = \"processed/\"+ test_df[\"audio_path\"] +\".mel.pt\"\n",
    "test_df['audio'] = test_df['audio_path'].apply(lambda x: os.path.join(raw_path, x.replace(\"audio/\", \"\") +'.wav'))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models import EncDecCTCModelBPE\n",
    "\n",
    "\n",
    "asr_model = EncDecCTCModelBPE.restore_from(restore_path=nemo_path, map_location=device, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/asr_language_modeling/ngram_lm/install_beamsearch_decoders.sh\n",
    "# chmod +x install_beamsearch_decoders.sh\n",
    "\n",
    "# !mkdir -p $HOME/nemo_decoders\n",
    "\n",
    "# # Run the install script using your user directory (no sudo needed)\n",
    "# !bash install_beamsearch_decoders.sh $HOME/nemo_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b65020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kenlm flashlight-text pyctcdecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ee4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optionally, set decoding strategy for faster inference\n",
    "# asr_model.decoding.strategy = 'greedy_batch'\n",
    "import copy\n",
    "decoding_config = copy.deepcopy(asr_model.cfg.decoding)\n",
    "decoding_config.strategy = \"beam\"\n",
    "decoding_config.beam.beam_size = 8  # You can adjust this number\n",
    "asr_model.change_decoding_strategy(decoding_config)\n",
    "\n",
    "# Transcribe\n",
    "results = asr_model.transcribe(test_df['audio'].to_list(),\n",
    "    batch_size=4, # Adjust batch size based on your GPU memory\n",
    "\n",
    ")\n",
    "\n",
    "results_list = []\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result.text)\n",
    "    results_list.append(result.text)\n",
    "\n",
    "test_df['transcription'] = results_list\n",
    "test_df['id'] = test_df.index\n",
    "test_df[[\"id\", \"transcription\"]].to_csv(f\"/shared/A_track/submission_epoch_{epoch}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, set decoding strategy for faster inference\n",
    "# asr_model.decoding.strategy = 'greedy_batch'\n",
    "\n",
    "# Transcribe\n",
    "results = asr_model.transcribe(test_df['audio'].to_list())\n",
    "\n",
    "results_list = []\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(result.text)\n",
    "    results_list.append(result.text)\n",
    "\n",
    "test_df['transcription'] = results_list\n",
    "test_df['id'] = test_df.index\n",
    "test_df[[\"id\", \"transcription\"]].to_csv(f\"/shared/A_track/submission_epoch_{epoch}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission file path\n",
    "submission_path = f\"/shared/A_track/submission_epoch_{epoch}.csv\"\n",
    "\n",
    "# Make sure kaggle is installed and API credentials are set up in ~/.kaggle/kaggle.json\n",
    "\n",
    "# Example: Upload to a Kaggle competition (replace 'your-competition-name' with the actual competition name)\n",
    "!kaggle competitions submit -c kinyarwanda-automatic-speech-recognition-track-a -f \"{submission_path}\" -m \"Submission for epoch {epoch} Automated from the script\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# with tarfile.open(\"/shared/track_b_audio.tar.gz\", \"w:gz\") as tar:\n",
    "#     # Add audio files from track_b_audio_files\n",
    "#     for fname in test_df[\"audio\"]:\n",
    "#         tar.add(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shared-envpreproces)",
   "language": "python",
   "name": "shared-envpreproces"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
