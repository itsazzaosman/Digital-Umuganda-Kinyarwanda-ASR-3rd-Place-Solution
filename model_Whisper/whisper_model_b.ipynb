{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b9e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f073a96f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88920ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"hf_xxx\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d722196",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /ocean/projects/cis250085p/shared/B_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20939f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711a4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# train_json_path = \"/shared/B_track/train.json\"\n",
    "# train_df = pd.read_json(train_json_path).T\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dev_json_path = \"/shared/B_track/dev_test.json\"\n",
    "# dev_df = pd.read_json(dev_json_path).T\n",
    "# dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d622ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "\n",
    "# Base path\n",
    "base_path = \"/shared/B_track\"\n",
    "\n",
    "# Load and convert split\n",
    "def load_split(split_json, audio_folder):\n",
    "    json_path = os.path.join(base_path, f\"{split_json}.json\")\n",
    "    \n",
    "    # Load JSON as DataFrame\n",
    "    df = pd.read_json(json_path).T  # transpose to fix orientation\n",
    "    \n",
    "    # Remove \"audio/\" prefix from path\n",
    "    df[\"audio\"] = df[\"audio_path\"].str.replace(\"audio/\", \"\", regex=False).apply(\n",
    "        lambda x: os.path.join(base_path, audio_folder, x + \".wav\")\n",
    "    )\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    data = df.to_dict(orient=\"records\")\n",
    "    \n",
    "    return Dataset.from_list(data).cast_column(\"audio\", Audio())\n",
    "\n",
    "# Build dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_split(\"train\", \"train_audios_b\"),\n",
    "    \"test\": load_split(\"test\", \"test_audios_b\"),\n",
    "    \"validation\": load_split(\"dev_test\", \"val_audios_b\"),\n",
    "})\n",
    "\n",
    "#Inspect\n",
    "# print(dataset)\n",
    "# print(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"voice_creator_id\", \"image_path\", \"image_category\", \"image_sub_category\", \"age_group\", \"gender\", \"project_name\", \"locale\", \"year\", \"duration\", \"location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad42d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a0e68",
   "metadata": {},
   "source": [
    "## A feature extractor which pre-processes the raw audio-inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fd4e5",
   "metadata": {},
   "source": [
    "### Load WhisperFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e071c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab637d8",
   "metadata": {},
   "source": [
    "### Load WhisperTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d65d679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large-v2\", task=\"translate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = dataset[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids #The output is a list of numbers\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa1c3e6",
   "metadata": {},
   "source": [
    "### Combine To Create A WhisperProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96897276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\", task=\"translate\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28355a28",
   "metadata": {},
   "source": [
    "### Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b46013",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataset[\"train\"][0].keys())\n",
    "\n",
    "display(dataset[\"train\"][0][\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada81cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = dataset[\"train\"][3]\n",
    "\n",
    "print(\"--- Original Sample Batch (before processing) ---\")\n",
    "print(f\"Keys: {sample_batch.keys()}\")\n",
    "print(f\"Audio details: {sample_batch['audio']}\")\n",
    "print(f\"Transcription: {sample_batch['transcription']}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08804380",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_batch = prepare_dataset(sample_batch)\n",
    "print(\"\\n--- Processed Sample Batch (after prepare_dataset) ---\")\n",
    "print(f\"Keys: {processed_batch.keys()}\")\n",
    "\n",
    "# Check the 'input_features'\n",
    "\n",
    "print(f\"Input Features (log-Mel spectrogram):\")\n",
    "print(f\"  Type: {type(processed_batch['input_features'])}\")\n",
    "print(f\"  Shape: {processed_batch['input_features'].shape}\") # This will typically be (80, N_frames) for Whisper, where 80 is the number of Mel bins.\n",
    "print(f\"  Example values (first few): {processed_batch['input_features'].flatten()[:5]}\") # Flatten to show linear values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6179a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b50a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 'labels'\n",
    "print(f\"Labels (tokenized text):\")\n",
    "print(f\"  Type: {type(processed_batch['labels'])}\")\n",
    "print(f\"  Length: {len(processed_batch['labels'])}\")\n",
    "print(f\"  Example IDs (first few): {processed_batch['labels'][:10]}\")\n",
    "print(f\"  Decoded labels: {tokenizer.decode(processed_batch['labels'])}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78823520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # For numerical operations, though your features are already numpy arrays\n",
    "\n",
    "# Assuming you have 'processed_batch' from your previous execution\n",
    "# If you just ran the prepare_dataset function, processed_batch should be available.\n",
    "\n",
    "# 1. Get the input_features (the Mel spectrogram)\n",
    "mel_spectrogram = processed_batch[\"input_features\"]\n",
    "\n",
    "print(f\"Shape of Mel Spectrogram: {mel_spectrogram.shape}\")\n",
    "\n",
    "# 2. Plot the Mel spectrogram\n",
    "plt.figure(figsize=(12, 6)) # Adjust figure size for better viewing\n",
    "\n",
    "# imshow displays the array as an image.\n",
    "# We use 'origin='lower'' to have lower frequencies at the bottom of the plot.\n",
    "# 'aspect='auto'' adjusts the aspect ratio to fit the figure.\n",
    "# 'cmap='viridis'' or 'magma' or 'inferno' are good colormaps for spectrograms.\n",
    "plt.imshow(mel_spectrogram, origin='lower', aspect='auto', cmap='viridis')\n",
    "\n",
    "plt.title('Log-Mel Spectrogram')\n",
    "plt.ylabel('Mel Bins')\n",
    "plt.xlabel('Time Frames')\n",
    "plt.colorbar(label='Log-Mel Energy') # Add a color bar to show value scale\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show()\n",
    "\n",
    "# You can also confirm the data type\n",
    "print(f\"Data type of Mel Spectrogram: {mel_spectrogram.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0526eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24045a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dataset' is your DatasetDict (e.g., containing 'train', 'test', 'validation' splits)\n",
    "\n",
    "# Process only the 'train' split for the first 100 examples\n",
    "# You apply .select() to the specific split (dataset[\"train\"])\n",
    "# And then you apply .map() to that selected subset.\n",
    "\n",
    "# It's good practice to create a new DatasetDict to store the processed splits\n",
    "processed_dataset = DatasetDict()\n",
    "\n",
    "processed_dataset[\"train\"] = dataset[\"train\"].select(range(100)).map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=dataset[\"train\"].column_names, # Use column_names from the specific split\n",
    "    num_proc=1,\n",
    "    desc=\"Debugging small subset\"\n",
    "\n",
    ")\n",
    "\n",
    "# If you want to process other splits similarly, you'd do it for each one:\n",
    "# processed_dataset[\"test\"] = dataset[\"test\"].select(range(50)).map( # Or just the full test set\n",
    "#     prepare_dataset,\n",
    "#     remove_columns=dataset[\"test\"].column_names,\n",
    "#     num_proc=4\n",
    "# )\n",
    "# And so on for 'validation' if you have it.\n",
    "\n",
    "print(\"Processed DatasetDict structure:\")\n",
    "print(processed_dataset)\n",
    "\n",
    "# To access the processed training data, you would now use processed_dataset[\"train\"]\n",
    "# It means \"apply this specific function (prepare_dataset) to every single example (or 'row') in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf05c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After successful processing:\n",
    "processed_dataset.save_to_disk(\"/shared/B_track/processed_100_samples\")\n",
    "\n",
    "# To load it later:\n",
    "from datasets import load_from_disk\n",
    "loaded_processed_dataset = load_from_disk(\"/shared/B_track/processed_100_samples\")\n",
    "print(loaded_processed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83930209",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8df0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio # Explicitly import torchaudio for audio loading\n",
    "from datasets import load_dataset, Audio, DatasetDict\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set logging level to info to see more details from Hugging Face\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Paths for your TRAINING pipeline test data\n",
    "FULL_TRAIN_JSON_FILE = \"./kinyarwanda_train_test_data.json\" # JSON file for your training subset\n",
    "TRAIN_AUDIO_FOLDER = \"./kinyarwanda_train_audio_data\"     # Folder containing audio files referenced in FULL_TRAIN_JSON_FILE\n",
    "\n",
    "# Paths for your VALIDATION pipeline test data\n",
    "FULL_VALIDATION_JSON_FILE = \"./validation.json\"           # Your existing validation JSON file\n",
    "VALIDATION_AUDIO_FOLDER = \"./kinyarwanda_validation_audio_data\" # Folder containing audio files referenced in FULL_VALIDATION_JSON_FILE\n",
    "\n",
    "# Column names in your JSON files (ensure these match exactly!)\n",
    "AUDIO_FILE_COLUMN_NAME = \"audio_path\"\n",
    "SOURCE_TEXT_COLUMN_NAME = \"kinyarwanda_text\"     # Column in train JSON for Kinyarwanda text\n",
    "TRANSLATION_TEXT_COLUMN_NAME = \"english_translation\" # Column in both JSONs for English translation\n",
    "\n",
    "# Model and language settings\n",
    "WHISPER_MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "TARGET_LANGUAGE_FOR_TRANSLATION = \"English\" # This is used for the processor's language setting\n",
    "TARGET_LANGUAGE_CODE = \"en\"               # ISO 639-1 code for the target language (English)\n",
    "\n",
    "# Expected number of samples for this pipeline test\n",
    "EXPECTED_TRAIN_SAMPLES = 80\n",
    "EXPECTED_VALIDATION_SAMPLES = 20\n",
    "\n",
    "# Output directory for this pipeline test run\n",
    "OUTPUT_DIR_PIPELINE_TEST = \"./whisper-kinyarwanda-pipeline-test\"\n",
    "\n",
    "# --- 1. Load the Raw Datasets (for Pipeline Test) ---\n",
    "# We assume these JSON files already contain the desired small number of samples.\n",
    "print(f\"Loading raw training dataset from {FULL_TRAIN_JSON_FILE}...\")\n",
    "raw_train_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=FULL_TRAIN_JSON_FILE,\n",
    "    split=\"train\" # Load the entire (small) training JSON file\n",
    ")\n",
    "\n",
    "print(f\"Loading raw validation dataset from {FULL_VALIDATION_JSON_FILE}...\")\n",
    "raw_validation_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=FULL_VALIDATION_JSON_FILE,\n",
    "    split=\"train\" # Load the entire (small) validation JSON file\n",
    ")\n",
    "\n",
    "# Verify loaded counts and warn if they don't match expectations\n",
    "if len(raw_train_dataset) != EXPECTED_TRAIN_SAMPLES:\n",
    "    print(f\"WARNING: Expected {EXPECTED_TRAIN_SAMPLES} training samples, but loaded {len(raw_train_dataset)} from {FULL_TRAIN_JSON_FILE}.\")\n",
    "if len(raw_validation_dataset) != EXPECTED_VALIDATION_SAMPLES:\n",
    "    print(f\"WARNING: Expected {EXPECTED_VALIDATION_SAMPLES} validation samples, but loaded {len(raw_validation_dataset)} from {FULL_VALIDATION_JSON_FILE}.\")\n",
    "\n",
    "\n",
    "print(f\"Raw training dataset loaded: {len(raw_train_dataset)} samples\")\n",
    "print(f\"Raw validation dataset loaded: {len(raw_validation_dataset)} samples\")\n",
    "\n",
    "# --- 2. Initialize Whisper Processor ---\n",
    "print(f\"Loading Whisper processor for {WHISPER_MODEL_NAME} with target language '{TARGET_LANGUAGE_FOR_TRANSLATION}' and task 'translate'...\")\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    WHISPER_MODEL_NAME,\n",
    "    language=TARGET_LANGUAGE_FOR_TRANSLATION,\n",
    "    task=\"translate\"\n",
    ")\n",
    "\n",
    "# --- 3. Define Preprocessing Function ---\n",
    "# This function will be applied to both training and validation datasets.\n",
    "# It now takes an `audio_folder_base` argument to correctly locate audio files.\n",
    "def prepare_dataset(batch, audio_folder_base):\n",
    "    # Construct full audio path using the provided base folder\n",
    "    audio_file_name = batch[AUDIO_FILE_COLUMN_NAME]\n",
    "    audio_path = os.path.join(audio_folder_base, audio_file_name)\n",
    "\n",
    "    # Load and resample audio data using torchaudio\n",
    "    try:\n",
    "        speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        # Ensure 16kHz sampling rate\n",
    "        if sampling_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "            speech_array = resampler(speech_array)\n",
    "        speech_array = speech_array.squeeze(0).numpy() # Convert to mono numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        # Return empty features/labels for this sample so it gets skipped by the data collator's masking\n",
    "        return {\"input_features\": [], \"labels\": []}\n",
    "\n",
    "    # Compute log-Mel spectrogram input features\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        speech_array,\n",
    "        sampling_rate=16000 # Always pass 16000 here after resampling\n",
    "    ).input_features[0]\n",
    "\n",
    "    # Tokenize the target language transcription (English) for the labels\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[TRANSLATION_TEXT_COLUMN_NAME],\n",
    "        language=TARGET_LANGUAGE_FOR_TRANSLATION, # Target language for labels\n",
    "        task=\"translate\"\n",
    "    ).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "# --- 4. Apply Preprocessing to Training and Validation Subsets ---\n",
    "print(\"Applying preprocessing to training subset...\")\n",
    "# Use a lambda to pass the specific audio folder to prepare_dataset\n",
    "processed_train_dataset = raw_train_dataset.map(\n",
    "    lambda batch: prepare_dataset(batch, TRAIN_AUDIO_FOLDER),\n",
    "    remove_columns=raw_train_dataset.column_names,\n",
    "    num_proc=os.cpu_count() if os.cpu_count() > 1 else 1, # Use multiple cores if available\n",
    "    desc=\"Preprocessing Kinyarwanda training subset for translation\"\n",
    ")\n",
    "\n",
    "print(\"Applying preprocessing to validation subset...\")\n",
    "# Use a lambda to pass the specific audio folder to prepare_dataset\n",
    "processed_validation_dataset = raw_validation_dataset.map(\n",
    "    lambda batch: prepare_dataset(batch, VALIDATION_AUDIO_FOLDER),\n",
    "    remove_columns=raw_validation_dataset.column_names,\n",
    "    num_proc=os.cpu_count() if os.cpu_count() > 1 else 1, # Use multiple cores if available\n",
    "    desc=\"Preprocessing Kinyarwanda validation subset for translation\"\n",
    ")\n",
    "\n",
    "print(f\"Processed training dataset: {len(processed_train_dataset)} samples\")\n",
    "print(f\"Processed validation dataset: {len(processed_validation_dataset)} samples\")\n",
    "\n",
    "# --- (Optional) Save Processed Subsets ---\n",
    "# It's good practice to save them even for a pipeline test, helps debugging\n",
    "PROCESSED_TRAIN_PATH = os.path.join(OUTPUT_DIR_PIPELINE_TEST, \"processed_train_subset\")\n",
    "PROCESSED_VALIDATION_PATH = os.path.join(OUTPUT_DIR_PIPELINE_TEST, \"processed_validation_subset\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_PIPELINE_TEST, exist_ok=True)\n",
    "processed_train_dataset.save_to_disk(PROCESSED_TRAIN_PATH)\n",
    "processed_validation_dataset.save_to_disk(PROCESSED_VALIDATION_PATH)\n",
    "print(f\"Processed subsets saved to {PROCESSED_TRAIN_PATH} and {PROCESSED_VALIDATION_PATH}\")\n",
    "\n",
    "# --- 5. Define Data Collator ---\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Filter out examples where audio loading failed (empty input_features or labels)\n",
    "        features = [f for f in features if f.get(\"input_features\") and f.get(\"labels\")]\n",
    "        if not features: # If no valid features left in batch\n",
    "            return {}\n",
    "\n",
    "        # pad input features (audio)\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # pad labels (transcriptions)\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# --- 6. Define Metrics (for BLEU score in translation) ---\n",
    "# For translation, BLEU is a more common metric than WER.\n",
    "# You'll need to install sacrebleu: pip install sacrebleu\n",
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "metric_wer = evaluate.load(\"wer\") # Keep WER for reference if needed, though BLEU is primary for translation\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Clean references for BLEU (sacrebleu expects a list of lists of references)\n",
    "    cleaned_label_str = [[label] for label in label_str]\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = metric_bleu.compute(predictions=pred_str, references=cleaned_label_str)\n",
    "\n",
    "    # Compute WER for comparison\n",
    "    wer_score = metric_wer.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"bleu\": bleu_score[\"score\"], \"wer\": 100 * wer_score}\n",
    "\n",
    "# --- 7. Load Whisper Model ---\n",
    "print(f\"Loading Whisper model {WHISPER_MODEL_NAME} for fine-tuning...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL_NAME)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# --- 8. Set Up Training Arguments ---\n",
    "print(\"Setting up training arguments for pipeline test...\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_PIPELINE_TEST,\n",
    "    per_device_train_batch_size=4, # Smaller batch size for small dataset\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=50, # Fewer warmup steps for fewer total steps\n",
    "    max_steps=200,   # Very few steps for a quick pipeline test (e.g., 200-500)\n",
    "                     # This means each of your 80 training samples will be seen a few times.\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100, # Save checkpoint more frequently for a short test\n",
    "    eval_steps=100, # Evaluate more frequently for a short test\n",
    "    logging_steps=10, # Log training progress very frequently\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"bleu\", # Monitor BLEU score for translation\n",
    "    greater_is_better=True,       # Higher BLEU is better\n",
    "    push_to_hub=False, # Set to True if you want to push this test model\n",
    ")\n",
    "\n",
    "# --- 9. Initialize the Trainer ---\n",
    "print(\"Initializing Seq2SeqTrainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,     # Use your pre-processed training subset\n",
    "    eval_dataset=processed_validation_dataset, # Use your pre-processed validation subset\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# --- 10. Start the Training! ---\n",
    "print(\"\\nStarting pipeline test training...\")\n",
    "trainer.train()\n",
    "print(\"\\nPipeline test training complete!\")\n",
    "\n",
    "# --- 11. (Optional) Final Evaluation and Inference Test ---\n",
    "print(\"\\nPerforming final evaluation on the processed validation dataset:\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "print(\"\\n--- Example Inference after Pipeline Test ---\")\n",
    "# Pick a sample from the raw validation dataset for a quick check\n",
    "if len(raw_validation_dataset) > 0:\n",
    "    sample_index_for_inference = 0 # You can change this index to test different samples\n",
    "    raw_sample = raw_validation_dataset[sample_index_for_inference]\n",
    "\n",
    "    # Load the raw audio array for the pipeline\n",
    "    audio_to_load_path = os.path.join(VALIDATION_AUDIO_FOLDER, raw_sample[AUDIO_FILE_COLUMN_NAME])\n",
    "    try:\n",
    "        input_audio_array, _ = torchaudio.load(audio_to_load_path)\n",
    "        input_audio_array = input_audio_array.squeeze(0).numpy() # Ensure mono numpy array\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load audio for inference test from {audio_to_load_path}: {e}\")\n",
    "        input_audio_array = None\n",
    "\n",
    "    if input_audio_array is not None:\n",
    "        from transformers import pipeline\n",
    "        asr_pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=trainer.model, # Use the model from the trainer (which is the best one loaded at the end)\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            device=0 if torch.cuda.is_available() else -1 # Use GPU if available (0 is first GPU, -1 is CPU)\n",
    "        )\n",
    "\n",
    "        # Set generation configuration for translation\n",
    "        gen_kwargs = {\"language\": TARGET_LANGUAGE_FOR_TRANSLATION, \"task\": \"translate\"}\n",
    "\n",
    "        result = asr_pipeline(input_audio_array, generate_kwargs=gen_kwargs)\n",
    "\n",
    "        print(f\"--- Sample {sample_index_for_inference} ---\")\n",
    "        print(f\"Original Kinyarwanda Text: {raw_sample.get(SOURCE_TEXT_COLUMN_NAME, 'N/A')}\") # Use .get() for safety\n",
    "        print(f\"True English Translation: {raw_sample[TRANSLATION_TEXT_COLUMN_NAME]}\")\n",
    "        print(f\"Predicted English Translation: {result['text']}\")\n",
    "    else:\n",
    "        print(\"Skipping inference test due to audio loading error.\")\n",
    "else:\n",
    "    print(\"No validation samples to perform inference test.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e7836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "\n",
    "# Define your data collator (copy-paste from previous steps if you still have it)\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # pad input features (audio)\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # pad labels (transcriptions)\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Load the pre-trained Whisper model and processor\n",
    "model_name = \"openai/whisper-large-v3\" # Or the size you chose\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language=\"Kinyarwanda\", task=\"translate\")\n",
    "\n",
    "# Instantiate the data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# Configure the model for generation (important for Seq2SeqTrainer)\n",
    "model.config.forced_decoder_ids = None # This is typical for ASR, as output length varies\n",
    "model.config.suppress_tokens = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
