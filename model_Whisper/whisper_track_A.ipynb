{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d01c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "# from datasets import Dataset\n",
    "from transformers import WhisperFeatureExtractor\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    # DataCollatorSpeechSeq2SeqWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "raw_path = \"/ocean/projects/cis250085p/shared/A_track/\"\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ffc28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bfcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- All the setup from before ---\n",
    "model_name = \"openai/whisper-large-v3\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "processor.tokenizer.set_prefix_tokens(language=\"kinyarwanda\", task=\"transcribe\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16,cache_dir = \"/ocean/projects/cis250085p/shared/A_track\") # Load in fp16 to save memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d7c3bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/A_track\n"
     ]
    }
   ],
   "source": [
    "%cd /ocean/projects/cis250085p/shared/A_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fb2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f80c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d1080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f3b1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "import datetime\n",
    "\n",
    "# Give it a completely new name to avoid any caching issues.\n",
    "class FinalDebuggingTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"\n",
    "        This is the definitive version with a print statement for debugging.\n",
    "        If we see the printout, we know this code is running.\n",
    "        \"\"\"\n",
    "        # --- DEBUGGING PRINT STATEMENT ---\n",
    "        # print(f\"--- EXECUTING FinalDebuggingTrainer.compute_loss at {datetime.datetime.now()} ---\")\n",
    "\n",
    "        # The rest of the logic remains the same.\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d0ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting the model to accept 80-channel input...\n",
      "✅ Model successfully adapted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_49223/2288983708.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FinalDebuggingTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FinalDebuggingTrainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,547,238,400 || trainable%: 0.2541\n",
      "Starting memory-efficient LoRA fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5380' max='112705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5380/112705 05:44 < 27:10:44, 1.10 it/s, Epoch 0.24/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>5.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.150300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.645300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Start Fine-Tuning\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting memory-efficient LoRA fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# resume_from_checkpoint=True,\u001b[39;49;00m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-5000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving final LoRA adapter...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33m./whisper-large-v3-lora-streaming-final\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mFinalDebuggingTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mThis is the definitive version with a print statement for debugging.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mIf we see the printout, we know this code is running.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- DEBUGGING PRINT STATEMENT ---\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# print(f\"--- EXECUTING FinalDebuggingTrainer.compute_loss at {datetime.datetime.now()} ---\")\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# The rest of the logic remains the same.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:818\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    817\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:818\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    817\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: Module._call_impl at line 1762 (21 times), Module._wrapped_call_impl at line 1751 (21 times), PeftModel.forward at line 818 (20 times), ConvertOutputsToFp32.__call__ at line 806 (15 times), autocast_decorator.<locals>.decorate_autocast at line 44 (15 times), convert_outputs_to_fp32.<locals>.forward at line 818 (15 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:818\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    817\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1694\u001b[39m, in \u001b[36mWhisperForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1690\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1691\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1692\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1712\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.proj_out(outputs[\u001b[32m0\u001b[39m])\n\u001b[32m   1714\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1529\u001b[39m, in \u001b[36mWhisperModel.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1522\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1523\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1524\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1525\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1526\u001b[39m     )\n\u001b[32m   1528\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1188\u001b[39m, in \u001b[36mWhisperDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1174\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1175\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1176\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1185\u001b[39m         cache_position,\n\u001b[32m   1186\u001b[39m     )\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1201\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:711\u001b[39m, in \u001b[36mWhisperDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[39m\n\u001b[32m    708\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(hidden_states)\n\u001b[32m    710\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m hidden_states = nn.functional.dropout(hidden_states, p=\u001b[38;5;28mself\u001b[39m.dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    720\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:520\u001b[39m, in \u001b[36mWhisperSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions, cache_position)\u001b[39m\n\u001b[32m    518\u001b[39m     value_states = past_key_value.value_cache[\u001b[38;5;28mself\u001b[39m.layer_idx]\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m.view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    521\u001b[39m     value_states = \u001b[38;5;28mself\u001b[39m.v_proj(current_states).view(bsz, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    522\u001b[39m     key_states = key_states.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1755\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n\u001b[32m   1753\u001b[39m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1756\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "# model = prepare_model_for_kbit_training(model)  # <-- COMMENT OUT OR DELETE THIS\n",
    "\n",
    "model = adapt_model_to_80_bins(model)\n",
    "\n",
    "\n",
    "# 3. CONFIGURE LORA (on the newly adapted model)\n",
    "# You can now proceed with your LoRA setup as before\n",
    "config = LoraConfig(r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "data_collator = robust_data_collator\n",
    "# Define Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-v3-lora-streaming\",\n",
    "    # output_dir=\"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-6000\",\n",
    "    per_device_train_batch_size=2,  # Adjust based on your VRAM\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True, # Already loaded model in fp16, but this ensures trainer uses it\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    # --- Important for streaming ---\n",
    "    # The dataloader will now be much more efficient\n",
    "    dataloader_num_workers=4, # Use multiple CPU cores to load data in parallel\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "\n",
    "        # --- NEW ARGUMENTS FOR EVALUATION ---\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,                       # Evaluate every 1000 steps\n",
    "    # load_best_model_at_end=True,          # Load the best model (lowest WER) at the end of training\n",
    "    metric_for_best_model=\"wer\",          # The metric to monitor for the best model\n",
    "    greater_is_better=False,             # For WER, a lower score is better\n",
    "    predict_with_generate=True,           # **CRITICAL**: This tells the Trainer to use the .generate() method for inference, which is required for Whisper.\n",
    "\n",
    ")\n",
    "\n",
    "trainer = FinalDebuggingTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    # The 'tokenizer' argument is deprecated, but we'll leave it for now\n",
    "    # as the warning is not critical to this error.\n",
    "    tokenizer=processor.feature_extractor,\n",
    "\n",
    "    eval_dataset=eval_dataset,          # <-- Pass the evaluation dataset\n",
    "    compute_metrics=compute_metrics,    # <-- Pass your metrics function\n",
    "    \n",
    "\n",
    ")\n",
    "# Start Fine-Tuning\n",
    "print(\"Starting memory-efficient LoRA fine-tuning...\")\n",
    "trainer.train(\n",
    "\n",
    "    # resume_from_checkpoint=True,\n",
    "    resume_from_checkpoint=\"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-5000\",\n",
    ")\n",
    "\n",
    "print(\"Saving final LoRA adapter...\")\n",
    "trainer.save_model(\"./whisper-large-v3-lora-streaming-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting the model to accept 80-channel input...\n",
      "✅ Model successfully adapted.\n",
      "Manually loading LoRA adapter from: /ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-6000\n",
      "\n",
      "Model and adapter loaded successfully.\n",
      "trainable params: 3,932,160 || all params: 1,547,238,400 || trainable%: 0.2541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.q_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from peft import PeftModel, LoraConfig\n",
    "import torch\n",
    "\n",
    "# (Your 'adapt_model_to_80_bins' function should be defined here)\n",
    "# (Your 'robust_data_collator' function should be defined here)\n",
    "# (Your 'FinalDebuggingTrainer' class should be defined here)\n",
    "\n",
    "# --- 1. Load the Base Model ---\n",
    "model_name = \"openai/whisper-large-v3\"\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "processor.tokenizer.set_prefix_tokens(language=\"kinyarwanda\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16,cache_dir = \"/ocean/projects/cis250085p/shared/A_track\") # Load in fp16 to save memory\n",
    "\n",
    "# --- 2. Perform the Model Surgery ---\n",
    "model = adapt_model_to_80_bins(model)\n",
    "\n",
    "# --- 3. Manually Load Your Trained LoRA Adapter ---\n",
    "# This is the key step. We are loading the progress from your best checkpoint.\n",
    "checkpoint_path = \"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-6000\"\n",
    "print(f\"Manually loading LoRA adapter from: {checkpoint_path}\")\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path, is_trainable=True)\n",
    "\n",
    "print(\"\\nModel and adapter loaded successfully.\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f80daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_49223/3232425285.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FinalDebuggingTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FinalDebuggingTrainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory-efficient LoRA fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='112705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    10/112705 00:07 < 28:06:40, 1.11 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Start Fine-Tuning\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting memory-efficient LoRA fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaving final LoRA adapter...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33m./whisper-large-v3-lora-streaming-final\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mFinalDebuggingTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, **kwargs)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mThis is the definitive version with a print statement for debugging.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mIf we see the printout, we know this code is running.\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- DEBUGGING PRINT STATEMENT ---\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# print(f\"--- EXECUTING FinalDebuggingTrainer.compute_loss at {datetime.datetime.now()} ---\")\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# The rest of the logic remains the same.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (loss, outputs) \u001b[38;5;28;01mif\u001b[39;00m return_outputs \u001b[38;5;28;01melse\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:818\u001b[39m, in \u001b[36mPeftModel.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    817\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1694\u001b[39m, in \u001b[36mWhisperForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1690\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1691\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1692\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1712\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.proj_out(outputs[\u001b[32m0\u001b[39m])\n\u001b[32m   1714\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1529\u001b[39m, in \u001b[36mWhisperModel.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1522\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1523\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1524\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1525\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1526\u001b[39m     )\n\u001b[32m   1528\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1188\u001b[39m, in \u001b[36mWhisperDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1174\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1175\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1176\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1185\u001b[39m         cache_position,\n\u001b[32m   1186\u001b[39m     )\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1201\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:708\u001b[39m, in \u001b[36mWhisperDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[33;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    705\u001b[39m \u001b[33;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    707\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    711\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    712\u001b[39m     hidden_states=hidden_states,\n\u001b[32m    713\u001b[39m     past_key_value=past_key_value,\n\u001b[32m   (...)\u001b[39m\u001b[32m    717\u001b[39m     cache_position=cache_position,\n\u001b[32m    718\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/functional.py:2910\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2902\u001b[39m         layer_norm,\n\u001b[32m   2903\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2908\u001b[39m         eps=eps,\n\u001b[32m   2909\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# data_collator = robust_data_collator\n",
    "# # Define Training Arguments\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"./whisper-large-v3-lora-streaming\",\n",
    "#     # output_dir=\"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-6000\",\n",
    "#     per_device_train_batch_size=2,  # Adjust based on your VRAM\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     learning_rate=1e-4,\n",
    "#     warmup_steps=100,\n",
    "#     num_train_epochs=5,\n",
    "#     fp16=True, # Already loaded model in fp16, but this ensures trainer uses it\n",
    "#     logging_steps=100,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=1000,\n",
    "#     # --- Important for streaming ---\n",
    "#     # The dataloader will now be much more efficient\n",
    "#     dataloader_num_workers=4, # Use multiple CPU cores to load data in parallel\n",
    "#     remove_unused_columns=False,\n",
    "\n",
    "\n",
    "#         # --- NEW ARGUMENTS FOR EVALUATION ---\n",
    "#     do_eval=True,\n",
    "#     eval_steps=100,                       # Evaluate every 1000 steps\n",
    "#     # load_best_model_at_end=True,          # Load the best model (lowest WER) at the end of training\n",
    "#     metric_for_best_model=\"wer\",          # The metric to monitor for the best model\n",
    "#     greater_is_better=False,             # For WER, a lower score is better\n",
    "#     predict_with_generate=True,           # **CRITICAL**: This tells the Trainer to use the .generate() method for inference, which is required for Whisper.\n",
    "\n",
    "# )\n",
    "\n",
    "# # Initialize Trainer\n",
    "# # trainer = CustomSeq2SeqTrainer(\n",
    "# #     args=training_args,\n",
    "# #     model=model,\n",
    "# #     train_dataset=dataset, # Pass the transformed dataset\n",
    "# #     data_collator=data_collator,\n",
    "# #     tokenizer=processor.feature_extractor,\n",
    "# # )\n",
    "\n",
    "# trainer = FinalDebuggingTrainer(\n",
    "#     args=training_args,\n",
    "#     model=model,\n",
    "#     train_dataset=dataset,\n",
    "#     data_collator=data_collator,\n",
    "#     # The 'tokenizer' argument is deprecated, but we'll leave it for now\n",
    "#     # as the warning is not critical to this error.\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "\n",
    "#     eval_dataset=eval_dataset,          # <-- Pass the evaluation dataset\n",
    "#     compute_metrics=compute_metrics,    # <-- Pass your metrics function\n",
    "    \n",
    "\n",
    "# )\n",
    "# # Start Fine-Tuning\n",
    "# print(\"Starting memory-efficient LoRA fine-tuning...\")\n",
    "# trainer.train(\n",
    "# )\n",
    "\n",
    "# print(\"Saving final LoRA adapter...\")\n",
    "# trainer.save_model(\"./whisper-large-v3-lora-streaming-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "387e3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from pickle import UnpicklingError\n",
    "import numpy as np # It's good practice to import numpy\n",
    "\n",
    "def robust_data_collator(features):\n",
    "    \"\"\"\n",
    "    This is the final, fully corrected version. It handles corrupted files AND\n",
    "    correctly handles the case where the saved files are NumPy arrays.\n",
    "    \"\"\"\n",
    "    valid_features = []\n",
    "    \n",
    "    # Part 1: Transformation\n",
    "    for feature in features:\n",
    "        try:\n",
    "            # 1. Load the object from disk (which we now know is a numpy array)\n",
    "            numpy_array = torch.load(raw_path + feature[\"file_path\"], weights_only=False)\n",
    "            \n",
    "            # --- THE FIX: First convert numpy array to torch tensor, THEN change dtype ---\n",
    "            audio_tensor = torch.from_numpy(numpy_array).to(torch.float16)\n",
    "            \n",
    "            # 2. Tokenize the transcription\n",
    "            tokenized_label = processor.tokenizer(feature[\"transcription\"]).input_ids\n",
    "            valid_features.append({\n",
    "                \"input_features\": audio_tensor,\n",
    "                \"labels\": tokenized_label\n",
    "            })\n",
    "            \n",
    "        except (EOFError, RuntimeError, UnpicklingError) as e:\n",
    "            warnings.warn(f\"Skipping corrupted file: {feature['file_path']} | Error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Part 2: Padding (only on the valid features)\n",
    "    if not valid_features:\n",
    "        warnings.warn(\"An entire batch of data was corrupted and has been skipped.\")\n",
    "        return {}\n",
    "        \n",
    "    input_features_list = [{\"input_features\": feature[\"input_features\"]} for feature in valid_features]\n",
    "    label_features_list = [{\"input_ids\": feature[\"labels\"]} for feature in valid_features]\n",
    "\n",
    "    batch = processor.feature_extractor.pad(input_features_list, return_tensors=\"pt\")\n",
    "    labels_batch = processor.tokenizer.pad(label_features_list, return_tensors=\"pt\")\n",
    "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "    if (labels[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "        labels = labels[:, 1:]\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4711fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: openai/whisper-large-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting the model to accept 80-channel input...\n",
      "✅ Model successfully adapted.\n",
      "Manually loading LoRA adapter from: /ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-6000\n",
      "Optimizing generation config for faster evaluation...\n",
      "\n",
      "Model and adapter loaded successfully.\n",
      "trainable params: 3,932,160 || all params: 1,547,238,400 || trainable%: 0.2541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model._orig_mod.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.encoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.encoder.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.12.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.13.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.14.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.15.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.16.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.17.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.18.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.19.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.20.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.21.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.22.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.23.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.24.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.25.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.26.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.27.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.28.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.29.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.30.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model._orig_mod.model.decoder.layers.31.encoder_attn.q_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from peft import PeftModel, LoraConfig\n",
    "import torch\n",
    "\n",
    "# (Your 'adapt_model_to_80_bins' function should be defined here)\n",
    "# (Your 'robust_data_collator' function should be defined here)\n",
    "# (Your 'FinalDebuggingTrainer' class should be defined here)\n",
    "\n",
    "# --- 1. Load the Base Model ---\n",
    "model_name = \"openai/whisper-large-v3\"\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "processor.tokenizer.set_prefix_tokens(language=\"kinyarwanda\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16,cache_dir = \"/ocean/projects/cis250085p/shared/A_track\") # Load in fp16 to save memory\n",
    "\n",
    "# --- 2. Perform the Model Surgery ---\n",
    "model = adapt_model_to_80_bins(model)\n",
    "\n",
    "# This might take a minute the first time it's called\n",
    "model = torch.compile(model) \n",
    "\n",
    "# --- 3. Manually Load Your Trained LoRA Adapter ---\n",
    "# This is the key step. We are loading the progress from your best checkpoint.\n",
    "checkpoint_path = \"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-6000\"\n",
    "print(f\"Manually loading LoRA adapter from: {checkpoint_path}\")\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path, is_trainable=True)\n",
    "\n",
    "# --- 2. THIS IS THE NEW PART: OPTIMIZE GENERATION CONFIG ---\n",
    "print(\"Optimizing generation config for faster evaluation...\")\n",
    "model.generation_config.num_beams = 1\n",
    "model.generation_config.do_sample = False\n",
    "\n",
    "print(\"\\nModel and adapter loaded successfully.\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1bd54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_49223/3735704561.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FinalDebuggingTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FinalDebuggingTrainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a new training run with pre-loaded adapter weights...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='75140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   23/75140 00:24 < 24:26:34, 0.85 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Define Training Arguments for the New Run ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # Use a new directory for this resumed run to avoid conflicts\n",
    "    output_dir=\"./whisper-large-v3-lora-resumed-run\", \n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4, # You can optionally lower this (e.g., 5e-5) since you're already in a good spot\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=5, # This will train for 5 *additional* epochs\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "\n",
    "    report_to=\"wandb\",\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "\n",
    "    # --- NEW ARGUMENTS FOR EVALUATION ---\n",
    "    eval_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=50,                       # Evaluate every 1000 steps\n",
    "    # load_best_model_at_end=True,          # Load the best model (lowest WER) at the end of training\n",
    "    metric_for_best_model=\"wer\",          # The metric to monitor for the best model\n",
    "    greater_is_better=False,             # For WER, a lower score is better\n",
    "    predict_with_generate=True,           # **CRITICAL**: This tells the Trainer to use the .generate() method for inference, which is required for Whisper.\n",
    "    generation_max_length=225,\n",
    "    # generation_num_beams=1,              # Use beam search for better quality\n",
    "    dataloader_num_workers=5,             # Use multiple CPU cores to load data in parallel\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    ")\n",
    "\n",
    "# --- 5. Initialize Trainer ---\n",
    "trainer = FinalDebuggingTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=robust_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# --- 6. Start the NEW Training Run ---\n",
    "# We do NOT use resume_from_checkpoint here.\n",
    "print(\"Starting a new training run with pre-loaded adapter weights...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "trainer.save_model(\"./whisper-large-v3-final-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2d793bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5897b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9de8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 1: Configuration - UPDATE THESE PATHS ---\n",
    "# ===================================================================================\n",
    "# The original model you fine-tuned\n",
    "MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "\n",
    "# Path to the LoRA adapter you trained and saved\n",
    "# This is the directory created by `trainer.save_model()`\n",
    "ADAPTER_PATH = \"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-4000\"\n",
    "\n",
    "# Path to your test data metadata file\n",
    "TEST_DATA_CSV = \"/ocean/projects/cis250085p/shared/A_track/dev_test.json\" # <-- IMPORTANT: Change this!\n",
    "\n",
    "# The base path where your raw audio files are stored, same as in training\n",
    "RAW_AUDIO_BASE_PATH = \"/ocean/projects/cis250085p/shared/A_track/\"\n",
    "\n",
    "# Configuration for the prediction run\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 2 # Adjust based on your GPU memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49845a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and processor...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WhisperForConditionalGeneration' object has no attribute 'enable_adapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:793\u001b[39m, in \u001b[36mPeftModel.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'PeftModel' object has no attribute 'enable_adapter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/tuners/lora/model.py:359\u001b[39m, in \u001b[36mLoraModel.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# defer to nn.Module's logic\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'LoraModel' object has no attribute 'enable_adapter'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# model = PeftModel.from_pretrained(base_model, ADAPTER_PATH).to(DEVICE)\u001b[39;00m\n\u001b[32m     19\u001b[39m model = PeftModel.from_pretrained(base_model, ADAPTER_PATH, is_trainable=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_adapter\u001b[49m() \n\u001b[32m     23\u001b[39m model.eval() \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[32m     25\u001b[39m base_model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:797\u001b[39m, in \u001b[36mPeftModel.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mbase_model\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/tuners/lora/model.py:363\u001b[39m, in \u001b[36mLoraModel.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'WhisperForConditionalGeneration' object has no attribute 'enable_adapter'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 2: Load Your Fine-Tuned Model and Processor ---\n",
    "# ===================================================================================\n",
    "print(\"Loading model and processor...\")\n",
    "\n",
    "# Load the processor\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "processor.tokenizer.set_prefix_tokens(language=\"kinyarwanda\", task=\"transcribe\")\n",
    "\n",
    "# Load the base model in float16 for faster inference\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    cache_dir = \"/ocean/projects/cis250085p/shared/A_track\"\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, ADAPTER_PATH).to(DEVICE)\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH, is_trainable=True)\n",
    "\n",
    "# model.enable_adapter() \n",
    "\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "base_model.eval()\n",
    "# --- OPTIONAL BUT RECOMMENDED: Apply optimizations from training ---\n",
    "# 1. Use greedy search for maximum speed\n",
    "# model.generation_config.num_beams = 1\n",
    "# model.generation_config.do_sample = False\n",
    "# 2. Compile the model if using PyTorch 2.0+\n",
    "# model = torch.compile(model)\n",
    "\n",
    "print(\"✅ Model loaded and configured for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffaf6b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: ./A_track/whisper-large-v3-lora-streaming/checkpoint-4000/adapter_model.safetensors\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xec'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m adapter_weights_path = \u001b[33m\"\u001b[39m\u001b[33m./A_track/whisper-large-v3-lora-streaming/checkpoint-4000/adapter_model.safetensors\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading weights from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_weights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m adapter_weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_weights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Let's inspect a key from one of the deeper layers in the decoder\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# This key should have been trained and have non-zero values\u001b[39;00m\n\u001b[32m     11\u001b[39m key_to_check = \u001b[33m\"\u001b[39m\u001b[33mbase_model.model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/serialization.py:1549\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1547\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1548\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/serialization.py:1797\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1794\u001b[39m         \u001b[38;5;66;03m# if not a tarfile, reset file offset and proceed\u001b[39;00m\n\u001b[32m   1795\u001b[39m         f.seek(\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m magic_number = \u001b[43mpickle_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic_number != MAGIC_NUMBER:\n\u001b[32m   1799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid magic number; corrupt file?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnpicklingError\u001b[39m: invalid load key, '\\xec'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your saved adapter weights file\n",
    "adapter_weights_path = \"./A_track/whisper-large-v3-lora-streaming/checkpoint-4000/adapter_model.safetensors\" \n",
    "\n",
    "print(f\"Loading weights from: {adapter_weights_path}\")\n",
    "adapter_weights = torch.load(adapter_weights_path, weights_only=False)\n",
    "\n",
    "# Let's inspect a key from one of the deeper layers in the decoder\n",
    "# This key should have been trained and have non-zero values\n",
    "key_to_check = \"base_model.model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight\"\n",
    "\n",
    "if key_to_check in adapter_weights:\n",
    "    weights = adapter_weights[key_to_check]\n",
    "    print(f\"\\nSuccessfully found key: {key_to_check}\")\n",
    "    print(\"Shape of weights tensor:\", weights.shape)\n",
    "    print(\"A small sample of the weights:\")\n",
    "    print(weights)\n",
    "    print(f\"\\nMean of absolute values: {weights.abs().float().mean()}\")\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Could not find the key '{key_to_check}' in the adapter file.\")\n",
    "    print(\"This indicates a serious problem with the saved checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de531253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f5c7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_df= pd.read_json(TEST_DATA_CSV).T\n",
    "# TD_df[\"file_path\"] = \"processed/\"+ TD_df[\"audio_path\"] +\".mel.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c46471",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_df[\"audio_path\"] = TD_df.audio_path.str.replace(\"audio/\",\"/ocean/projects/cis250085p/shared/track_a_audio_files/\")  +\".wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec4492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ocean/projects/cis250085p/shared/track_a_audio_files/1739532284-OogTF7X5UsTPNsR9q4GLZYcJiKB2.wav'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TD_df.audio_path.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39af103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from: /ocean/projects/cis250085p/shared/A_track/dev_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4632/4632 [06:42<00:00, 11.52 examples/s]\n",
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4632/4632 [13:30<00:00,  5.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test dataset prepared with 4632 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 3: Load and Prepare the Test Dataset ---\n",
    "# ===================================================================================\n",
    "print(f\"Loading test dataset from: {TEST_DATA_CSV}\")\n",
    "\n",
    "# Load the metadata\n",
    "test_dataset = Dataset.from_pandas(TD_df)\n",
    "\n",
    "# Function to prepare a single example for the model\n",
    "# It loads the audio and converts it into the 'input_features' the model expects\n",
    "def prepare_dataset(example):\n",
    "    audio_path = example[\"audio_path\"]\n",
    "    try:\n",
    "        # --- THIS IS THE NEW, MORE ROBUST METHOD ---\n",
    "        # 1. Load audio directly with torchaudio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # 2. The Whisper processor will handle resampling to 16kHz automatically\n",
    "        # We pass the raw waveform and its original sample rate\n",
    "        input_features = processor(\n",
    "            waveform.squeeze(), # Remove channel dimension\n",
    "            sampling_rate=sample_rate\n",
    "        ).input_features[0]\n",
    "        # --- END OF NEW METHOD ---\n",
    "\n",
    "        # 3. Convert to float16 to match the model\n",
    "        example[\"input_features\"] = torch.from_numpy(input_features).to(torch.float16)\n",
    "\n",
    "    except Exception as e:\n",
    "        # This will now catch any errors from torchaudio or the processor\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        example[\"input_features\"] = None # Mark as None to filter later\n",
    "        \n",
    "    return example\n",
    "\n",
    "# Apply the preparation function\n",
    "test_dataset = test_dataset.map(prepare_dataset, num_proc=4)\n",
    "# Filter out any samples that failed to load\n",
    "# test_dataset = test_dataset.filter(lambda example: example[\"input_features\"] is not None)\n",
    "\n",
    "print(f\"✅ Test dataset prepared with {len(test_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                          | 0/290 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                                                                                               | 2/290 [00:45<1:48:36, 22.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run prediction\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \u001b[38;5;66;03m# Disable gradient calculation for inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     predicted_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Decode the predicted IDs to text\u001b[39;00m\n\u001b[32m     27\u001b[39m transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:823\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m    822\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n\u001b[32m    768\u001b[39m (\n\u001b[32m    769\u001b[39m     seek_sequences,\n\u001b[32m    770\u001b[39m     seek_outputs,\n\u001b[32m    771\u001b[39m     should_skip,\n\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n\u001b[32m    773\u001b[39m     model_output_type,\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n\u001b[32m    948\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1694\u001b[39m, in \u001b[36mWhisperForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1690\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1691\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1692\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1712\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.proj_out(outputs[\u001b[32m0\u001b[39m])\n\u001b[32m   1714\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1529\u001b[39m, in \u001b[36mWhisperModel.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1522\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1523\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1524\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1525\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1526\u001b[39m     )\n\u001b[32m   1528\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1188\u001b[39m, in \u001b[36mWhisperDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1174\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1175\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1176\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1185\u001b[39m         cache_position,\n\u001b[32m   1186\u001b[39m     )\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1201\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 4: Run the Prediction Loop ---\n",
    "# ===================================================================================\n",
    "# The data collator just needs to organize the batch\n",
    "def collate_fn(features):\n",
    "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "    batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "    return batch\n",
    "\n",
    "# Create a DataLoader for efficient batching\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "predictions = []\n",
    "references = test_dataset[\"transcription\"] # Get all ground truth transcriptions\n",
    "\n",
    "print(\"Running predictions on the test set...\")\n",
    "# Loop through the test data with a progress bar\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # Move batch to the GPU\n",
    "    inputs = batch[\"input_features\"].to(DEVICE)\n",
    "\n",
    "    # Run prediction\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        predicted_ids = model.generate(inputs)\n",
    "\n",
    "    # Decode the predicted IDs to text\n",
    "    transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)\n",
    "    predictions.extend(transcriptions)\n",
    "\n",
    "print(\"✅ Prediction loop complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd4bb916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Akajerekani gateretse hasi ku butaka gapfundikiye neza, gapfundiki umuvuniko w'umweru n'akajerekani karasa n'umweru, akajerekani karimo amata.\",\n",
       " 'Uburyo bwo kwishyura hakoreshejwe ikoranabuhanga, kubitsa, kubikuza, kohereza amafaranga, ukoresheje terefone ngendanwa.',\n",
       " \"Hano hari abantu batatu babiri muri bo ni ab'igitsina gabo, undi umwe usigaye ni uw'igitsina gore, bicaye ku ntebe z'ibara ry'umweru ndetse imbere yabo bari kunywa amata mu birahure.\",\n",
       " \"Abantu benshi bahagaze imbere y'inyubako ifite amarangi y'umutuku n'umweru, yanditseho amagambo ari mu rurimi rw'icyongereza ari mu ibara ry'umutuku, abagabo barimo bambaye imyenda itandukanye, hari bambaye amashati y'umweru ndetse n'amapantaro y'umukara, hari n'abambaye amakote na karavate, hari n'uwambaye ingofero na rinete.\",\n",
       " \"Ahantu heza hafite isuku ndetse n'amacumbi wasohokera ukaruhuka, hari amazi meza akorerwamo imyidagaduro ndetse na siporo ziruhura imiruho.\",\n",
       " \"Abantu batwara amapikipiki by'umwuga bahetse abagore ku mapikipiki yabo, bose bahagaze iruhande bahindukiye bari kuvugisha abo bahetse ku mapikipiki.\",\n",
       " \"Abantu benshi cyane, bafite uruhu rwera, bambaye imyenda itandukanye, harimo umwe wicaye ndetse n'abandi bahagaze, bari kureba ku meza, iteretseho ibintu bitandukanye...\",\n",
       " \"Abagabo babiri bicaye ahantu hamwe, umwe yambaye ishati y'umweru imbere ye hari mudasobwa, undi yambaye ishati irimo amabara atandukanye n'ipantaro y'umukara ndetse afite agakombe mu ntoki ari kunywa.\",\n",
       " \"Inshange ya emutiyeni irimo uburyo bwo kwishyura amafaranga serivisi za emutiyeni kohereza amafaranga n'ukuntu amafaranga yoherezwa mu Bugande.\",\n",
       " \"Waba ufite amafaranga y'amanyamahanga? Ni byiza kugana forex bureau bakaguhamo amanyarwanda, cyangwa se niba umuntu yayakohereje kuri telefone wo hanze ni byiza kugana Western union bakayakubikurira.\",\n",
       " \"Umugabo wambaye umupira uri mu ibara ry'umukara, wanditseho polise mu ibara ry'umweru, uhagaze ku kazu ka emutiyene kari mu ibara ry'umuhondo, gatanga serivise zitandukanye, yaba kubitsa, kohereza, kugura amayinitse, ndetse no kugurisha amayinite. Mu kuboko kw'iburyo kwe hakaba haparitsemo cyangwa se hari amakarito ari mu ibara ry'ubururu.\",\n",
       " \"Isoko ricururizwamo ibicuruzwa bitandukanye harimo ibihingwa, imyaka nk'ibishyimbo, ibigori, ingano, ndetse n'ibindi biribwa nk'amasaka, ibitoki ndetse n'ibigori, iri soko rifite n'utundi ducururizwamo, ibikoresho byo mu rugo nk'amabase n'imikubuzo ndetse n'ibikoresho byo mu rugo.\",\n",
       " 'Benshi mu baturage bafite amafaranga ahagije bakunze kujya mu mabanki cyangwa ku mirongo igiye itandukanye bakoresha mu guhamagara cyangwa kugura interineti bagiye gufata inguzanyo kugira ngo bizinesi zabo zitere imbere.',\n",
       " 'Uburyo bwo kohererezanya amafaranga aturutse wohereza mumahanga cyangwa se wohererezwa ukoresheje uburyo bwa manipura.',\n",
       " \"Mu nyubako nyinshi z'amahoteli ugenda uhasanga ibikorwa byinshi bigiye bitandukanye bigufasha kuruhuka aho ushobora kuhasanga nk'ubwogero ushobora kujya mu mazi ukaba waruhukiramo ndetse ukahasanga naho wakwicara hirengeye kandi heza hatekanye.\",\n",
       " \"Inzu y'ubucuruzi irimo akabati gateretsemo ibyo kunywa bitandukanye, ji y'Inyange, coca n'imyembe na za enaji, harimo n'icyuma gikusanya amata na teremusi n'indobo iteretsemo amandazi n'umukobwa wicaye ku ntebe n'umugore wicaye ku ntebe arimo kunywa.\",\n",
       " \"Aho bavunjira amafaranga y'amanyamahanga, ushobora kuzana amadolari bakakuvunjira bakaguha amanyarwanda ndetse wanaba ushaka amadolari ufite amanyarwanda nabwo bakakuvunjira bakaguha agaciro kayo.\",\n",
       " 'Abantu babiri umwe akaba afashe telefone muntoki, bakaba bari kwerekana uburyo bushobora kwifashishwa hakoreshejwe ikoranabuhanga, hanyuma umuntu akaba yakwishyura ibicuruzwa bitandukanye, cyangwa se akabasha kohereza amafaranga no kuyakira.',\n",
       " \"Ndabona muri butike harimo ibinyobwa by'amoko menshi biri muri firigo, aho biba bikonjeshwa akenshi bino ni n'ibyo mu nganda.\",\n",
       " \"Amafaranga ari mu bwoko bw'inoti akaba ariho ururimi rw'igishinwa ndetse n'umugabo umenyerewe muri icyo gihugu, akaba ariho imibare igenda igaragaza agaciro kayo, ndetse n'uburyo akoreshwa.\",\n",
       " \"Umugabo wambaye agapira k'umweru ndetse n'ishati y'amaboko maremare,  akaba ari kumwe n'umugore wambaye umupira w'amaboko maremare bombi bakabamo baraseka, uyu mugore afite urufunguzo mu ntoki,  bukaba ari ubutumwa bw'ikigo gitanga inguzanyo kikubwira ko wabona ubu ngubu inzu yawe ukagenda wishyura gake gake.\",\n",
       " \"Icyumba gikorerwamo ubucuruzi bw'ibintu bitandukanye bikoreshwa mu buzima bwa buri munsi mu rugo, harimo ibiribwa n'ibikoresho by'isuku bitondetse neza mu tubati dusize irangi ry'umweru twabigenewe ku buryo abaguzi babasha kwisanzura bakareba ibyo bakeneye bagashima. \",\n",
       " 'Umudamu wambaye ikanzu yiganjemo amabara atandukanye, wambaye igitambaro, wicaye ku ntebe, imbere ye hakaba hari ameza ndetse ateretse n\\'igikombe cy\\'umweru cy\\'amata. Iruhande rw\\'ibumoso hakaba hari undi mugabo wambaye ishati y\\'umukara ndetse n\\'ingofero y\\'ubururu, inyuma hakaba hari icyapa cyanditseho \"hano hari amata meza y\\'inka\". ',\n",
       " 'Umuntu ufashe amafaranga mu ntoki, amafaranga atagirira umumaro mu kuyahahisha ibiribwa, ibyo kwambara, ndetse bakaba bagaragaza yuko ushobora kuyavunjisha, aho ujyana amafaranga runaka, ayo ushaka bakaba bayaguha.',\n",
       " \"Capati ndetse n'igikombe cy'amata umuntu agiye kubinywa kandi arabifata nka bureke fasiti .  Capati ziraryoha cyane abanyarwanda benshi bakunda capati.\",\n",
       " \"Imodoka, urupangu, ikamyo, icyuma, amatara, ikirango cy'imodoka, ibirahure.\",\n",
       " \"Inzu y'ubucuruzi icururizwamo ibinyobwa bisembuye n'ibidasembuye, umukobwa wambaye umupira uri mu ibara ry'umukara afite akamwenyu ku maso, afite igikoresho k'ikoranbuhanga kifashishwa mukwishyura ukoresheje ikarita\",\n",
       " \"Icyapa kiri munyandiko y'ururimi rw'amahanga, cyandikishijweho amagambo manini cyane ari mw'ibara ry'umweru ariko aho ari harimo ibara ry'umutuku. Hariho nimero ushobora guhamagaraho kugirango bakuyobore.\",\n",
       " \"Ahangaha turabona icyapa ni inshuti ni ko babita ariko baba bashinzwe gufata amafaranga yawe uzanye ari abanyamahanga bakaguherezamo amanyarwanda cyangwa se ukazana abanyamahanga abaye ariyo ufite bakaguhereza amanyarwanda cyangwa ukazana amanyarwanda bakaguhereza abanyamahanga bitewe n'aho ugiye kujya guhahira cyangwa icyo ugiye kuyakoresha.\",\n",
       " \"Abazungu ubona ko bafite ibintu baje  guhaha birimo imboga, inyanya, amashu, ndetse n'ibindi ipantaro, inkweto nziza, imisatsi ifunze rwose urabona ko ateruye n'indabyo arimo aragena\",\n",
       " \"Umuntu wambaye umupira w'umutuku ndetse n'itaburiya ijya gusa umukara, hirya yiwe hari undi wambaye umupira ujya gusa umweru, ndetse yambariyemo agapira kakajya gusa umukara, imbere yiwe hari amacupa abiri, hari icupa ry'amavuta y'ubuto, ndetse n'irindi cupa risa umutuku.\",\n",
       " \"Abashinzwe ubuzima mu Rwanda, bashishikariza abacuruzi bacuruza ibintu byerekanye n'amatakugira isuku; ndetse bagakorera isuku cyane ahantu batunganyiriza amata, bivugwa ko amata, iyo agiyemo umwanda ashobora gutera indwara zikabije, zirimo na korera.\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "292d801b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akajerekani ketereze aasi kutaka kepfundi tieneza kepfundi tiju mufundiko u nyeru na akajerekani klasa na u nyeru akajerekani karimo amata',\n",
       " 'uwurijobu kweishira hakureishishge ikorana wuhanga kubi ita kubi ikuza kuhiriza ama faranga ukureishishge telefonu jendan',\n",
       " 'hano hari awa nu batatu babiri muri vo na vijitsi nagavo undi ume usigaye nu vijitsi nagori bicha iku newe zivarariji ume eru ndete imbere ya vo barikungwa amata mwira huri',\n",
       " 'aba anu benshi wa gaze imbere iinyu baku fite ya maranji yumu tuku nu mngeru ya anditseo ama gamba rimu rimu ligu icho njereza harimu ngibara li jumu tuku aba gawa warimo wa mbae imienda ita ndokanya rabamba ya mashati yu mngeru dete na mapano yumu kaina mbae ama kote na klavate hainu wa mbae ingofero na rinete',\n",
       " 'anoheza haftisukumdece na machombe wasoche rao karuhu kari amazmeza kwerikomu imetafatru ndetze na sporozi ruhura umuliza',\n",
       " 'aba anu watuara ama pichi pichi pichumunga wa heze abagode kuma pichi pichi yao vose waagaze iruhande wa hindu chie warikufu jisha awa wa heze kuma pichi pichi',\n",
       " 'awano vwenti chani wafite urvurguwela wambaye imienda itando kanyi harimo umge wichae ndetina wandi wahagaze warikule wakumeza iterezeho ibienu bitando kanyi',\n",
       " 'aba gawa wabilibicha ya hanu hamge umge ya ambaye ishati yungeru imberee harimudasobga undi ya ambaye ishati ilimamawaratandukanye nipanaroyomukara detse afitagakombe mwonochi harikongwa',\n",
       " 'mshange ya mtn ilimu uulijewa kushira mafaranga servise za mtn kwa ireza mafaranga nukunu ama faranga yohirizu wa ugandi',\n",
       " 'mfita mafranga ya maja mahanga nibijiza kukana forex impiro baka guha ammo ammajarukwa anda changu wase niifu mundwe kuhirijekuri telefoni uhanse nibijiza kukana western union baka guhu hikuuliru',\n",
       " 'mbuga wamba mwapiruri mni wararijumu karawandi tse opo risemi warariju ungeru uwaga ziku kazukami teneka mni wararijumu wadoka tanga saivise stamu kanyi nyawa kubitsa kuhereza kugura mainite ntete na kugurisha mainite mkuwako kivujokea kawa hapa rite mochangwa sehari ama karitari mni warariju ururu',\n",
       " 'kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo',\n",
       " 'wenchimu waturajiwe nzifitea mufuranga ajiria wakunza kujia mwawangi changa kumilongo jitandu hii wakuresha mungama gara changu wa brand trinity wajie gufata mungu za nye kujirango bizinesi zao vizitere imbere',\n",
       " 'uvurijo uko heririza nyama faraanga atutse uheriza maanga changwa seo heririzikwa ukureshi uvurijo kama nikira',\n",
       " 'mwanyuba kujenshi za mahoteli ujendo hasanga ibikoruga vijenshi vijiebitandu kani vigufasha kuru huku aho ushovara kuasanga mwabugodjero ushovara kujamumazi ukava waruhu chiramu leze uka hasanga na hoa kuichara hile njeye kandi heza hatekani',\n",
       " 'nzui vuchuruzi idima kabati ga teletemo ibjokungo bitanuka nye amazi jriinyanje kuka nimiye mbe naza enaje halimo nichu majikusa nyamata na terimonsi njindo biteritema mandazi numu kubwa itre kunebe numu ugore itre kunebe alimokungo',\n",
       " 'ahuwa bafunjira ama faranga ya manya mahanga ushaura kuzana amadorari baka kufunjira baka guha amanya guanda ndetsewa anabusha akamadorari ufita manya guanda na mkuba kufunjira baka guha gachiro kayo',\n",
       " 'haba anuabiri umwe akawafasha telefone mwono chi wakawari kwele kana uvulijowu shoro kufashishkwa hakoeshi kwa nabuhanga hanyuma mwono akawaya kuhishula ushuzgo vitandu kanyi changosi akawasha kwele za mafuranga nuku ya achira',\n",
       " 'ndiyogo namu mwari butike harimu divinyo mga yamu kumeshi tiri mwari sirigwa aovyo aviko njishkwa kenshi binonini pia mungani',\n",
       " 'amafranga arimu kukuginoti akawariho ururi miru kujishirua dete nu mugabu menyelewe moricho jihuku akawariho mivarijendi gara gaza agachiru kayo dete nu urijo akurishku',\n",
       " 'umukawa wambaya kapila kumweli nzete endisha hati ya mawako marimari akawari kumweli mwambaye mwapila mawako marimari huwambi wakai mwabalaseke kumwela kite ufunguza mwunochi ukawa aru utumga pichi ugojitanga iguzanyo chiku biela kuwa una unguvi mzui yao kajenda uishiragache gaji',\n",
       " 'echumba jikorelgo mo uvuchuruzi gibi nhu bitanduka nye vikoresh kwa mbizima uga ugrimunzi murugo harimo ibidibuga nivikoresho vjisuku bitonde tsenesa mutu vati dosizira njeri juu mwe eru tukwabi jenewe kuburidio abaguzi wabasha kuisanzura wakarebi vjo vacheneye bagashimu',\n",
       " 'mwadamu wamba ikanzu iga njumama waratandu kanya wamba ijitambaru uicha ikunebe mbere ya kawara meza nditse atiritsa nijikombe chungeru chamata ndiyo handelgu ya mwosogu ya kawaru ndi mga wamba ishati umukara nditse ninguofiru uigururu ndiyo maya kawari cha pachanditse wahanu harama ta meza inga',\n",
       " 'umuunu fashia mafuranga munuchi mafuranga tujirumaro mkuya haisha ibiriuga ibijokambara ndetanga wakawagara gazayuko usho rakuya vunjisha aho ujana mafuranga runaka gayushaka wakawaya guha',\n",
       " 'chapatis dekza nijikwembe cha mata omono ajie kubimwa kandi arabifata na breakfasti chapatis zirajio hachani awanya guandavenshi wakunda chapati',\n",
       " 'e modoc rupangu e camiu e ciuma e matar ce rama te ai ma doc e biraguri',\n",
       " 'churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churwa churwa chur',\n",
       " 'ichiapa chiri mwenyandiko yurulimiru kwa mahangu chandi chisigweho amagambo manini chane alijibu mibaradiju mngeru aliku aho alibi halimibaradiju mutuku hariko nimero shobora kwa magara ho kujirango bakuyobodi',\n",
       " 'ahangaha tulavani cha apa ninshuti nikowa vita arikowa nwa wa shingi kufata amafara anga ya uzanye ara maja mahanga baka guheriza mo amanya luguanda changu seo kaza na maja mahanga wa riyofite baka guheriza maja luguanda cheo kaza na maja luguanda baka guheriza maja mahanga bitewe na awa ujie kuja guhaira changu seo ujie kuja kuresho',\n",
       " 'uzo mkubwa nako iftivina juu kwa abidi mbingi mboga inyanya amashu tetseni mindi apanolo imaitanziza inyani misa tsefu unziru kwa sarana katerie ilinda jwa ajma rajenda',\n",
       " 'munu wambaye umupira umutuku ndetse ni taburia ijagusa umukara hirijaiwe harundi wambaye umupira ujagusungeru ndetse ya ambarie mwaga pira kajagusumukara imberiwe haramachupa abiri haruchupa jamavuta yubuto ndetse ni ndichupa risa umutuku',\n",
       " 'abashinzi kufuzi mamurgu anda bashi shikariza wa churuzi wa churuzi vijini mjerechi ya njina mata kujiri suku nitsewe kakuriri suku chane aha anu watunga njina mata bifukwa kwa amatia jimu mga anda ashubara kutiri nguara njizika vijie zilimona kuriri']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final Word Error Rate (WER)...\n",
      "\n",
      "🎉 Final Test WER: 1.4067 🎉\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 5: Calculate and Display the Final WER ---\n",
    "# ===================================================================================\n",
    "print(\"Calculating final Word Error Rate (WER)...\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "final_wer = wer_metric.compute(predictions=predictions, references=references[:32])\n",
    "\n",
    "print(f\"\\n🎉 Final Test WER: {final_wer:.4f} 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff7b8075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final Word Error Rate (WER)...\n",
      "\n",
      "🎉 Final Test WER: 1.4067 🎉\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 5: Calculate and Display the Final WER ---\n",
    "# ===================================================================================\n",
    "print(\"Calculating final Word Error Rate (WER)...\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "final_wer = wer_metric.compute(predictions=predictions, references=references[:32])\n",
    "\n",
    "print(f\"\\n🎉 Final Test WER: {final_wer:.4f} 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f972a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akajerekani ketereze aasi kutaka kepfundi tieneza kepfundi tiju mufundiko u nyeru na akajerekani klasa na u nyeru akajerekani karimo amata',\n",
       " 'uwurijobu kweishira hakureishishge ikorana wuhanga kubi ita kubi ikuza kuhiriza ama faranga ukureishishge telefonu jendan',\n",
       " 'hano hari awa nu batatu babiri muri vo na vijitsi nagavo undi ume usigaye nu vijitsi nagori bicha iku newe zivarariji ume eru ndete imbere ya vo barikungwa amata mwira huri',\n",
       " 'aba anu benshi wa gaze imbere iinyu baku fite ya maranji yumu tuku nu mngeru ya anditseo ama gamba rimu rimu ligu icho njereza harimu ngibara li jumu tuku aba gawa warimo wa mbae imienda ita ndokanya rabamba ya mashati yu mngeru dete na mapano yumu kaina mbae ama kote na klavate hainu wa mbae ingofero na rinete',\n",
       " 'anoheza haftisukumdece na machombe wasoche rao karuhu kari amazmeza kwerikomu imetafatru ndetze na sporozi ruhura umuliza',\n",
       " 'aba anu watuara ama pichi pichi pichumunga wa heze abagode kuma pichi pichi yao vose waagaze iruhande wa hindu chie warikufu jisha awa wa heze kuma pichi pichi',\n",
       " 'awano vwenti chani wafite urvurguwela wambaye imienda itando kanyi harimo umge wichae ndetina wandi wahagaze warikule wakumeza iterezeho ibienu bitando kanyi',\n",
       " 'aba gawa wabilibicha ya hanu hamge umge ya ambaye ishati yungeru imberee harimudasobga undi ya ambaye ishati ilimamawaratandukanye nipanaroyomukara detse afitagakombe mwonochi harikongwa',\n",
       " 'mshange ya mtn ilimu uulijewa kushira mafaranga servise za mtn kwa ireza mafaranga nukunu ama faranga yohirizu wa ugandi',\n",
       " 'mfita mafranga ya maja mahanga nibijiza kukana forex impiro baka guha ammo ammajarukwa anda changu wase niifu mundwe kuhirijekuri telefoni uhanse nibijiza kukana western union baka guhu hikuuliru',\n",
       " 'mbuga wamba mwapiruri mni wararijumu karawandi tse opo risemi warariju ungeru uwaga ziku kazukami teneka mni wararijumu wadoka tanga saivise stamu kanyi nyawa kubitsa kuhereza kugura mainite ntete na kugurisha mainite mkuwako kivujokea kawa hapa rite mochangwa sehari ama karitari mni warariju ururu',\n",
       " 'kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo',\n",
       " 'wenchimu waturajiwe nzifitea mufuranga ajiria wakunza kujia mwawangi changa kumilongo jitandu hii wakuresha mungama gara changu wa brand trinity wajie gufata mungu za nye kujirango bizinesi zao vizitere imbere',\n",
       " 'uvurijo uko heririza nyama faraanga atutse uheriza maanga changwa seo heririzikwa ukureshi uvurijo kama nikira',\n",
       " 'mwanyuba kujenshi za mahoteli ujendo hasanga ibikoruga vijenshi vijiebitandu kani vigufasha kuru huku aho ushovara kuasanga mwabugodjero ushovara kujamumazi ukava waruhu chiramu leze uka hasanga na hoa kuichara hile njeye kandi heza hatekani',\n",
       " 'nzui vuchuruzi idima kabati ga teletemo ibjokungo bitanuka nye amazi jriinyanje kuka nimiye mbe naza enaje halimo nichu majikusa nyamata na terimonsi njindo biteritema mandazi numu kubwa itre kunebe numu ugore itre kunebe alimokungo',\n",
       " 'ahuwa bafunjira ama faranga ya manya mahanga ushaura kuzana amadorari baka kufunjira baka guha amanya guanda ndetsewa anabusha akamadorari ufita manya guanda na mkuba kufunjira baka guha gachiro kayo',\n",
       " 'haba anuabiri umwe akawafasha telefone mwono chi wakawari kwele kana uvulijowu shoro kufashishkwa hakoeshi kwa nabuhanga hanyuma mwono akawaya kuhishula ushuzgo vitandu kanyi changosi akawasha kwele za mafuranga nuku ya achira',\n",
       " 'ndiyogo namu mwari butike harimu divinyo mga yamu kumeshi tiri mwari sirigwa aovyo aviko njishkwa kenshi binonini pia mungani',\n",
       " 'amafranga arimu kukuginoti akawariho ururi miru kujishirua dete nu mugabu menyelewe moricho jihuku akawariho mivarijendi gara gaza agachiru kayo dete nu urijo akurishku',\n",
       " 'umukawa wambaya kapila kumweli nzete endisha hati ya mawako marimari akawari kumweli mwambaye mwapila mawako marimari huwambi wakai mwabalaseke kumwela kite ufunguza mwunochi ukawa aru utumga pichi ugojitanga iguzanyo chiku biela kuwa una unguvi mzui yao kajenda uishiragache gaji',\n",
       " 'echumba jikorelgo mo uvuchuruzi gibi nhu bitanduka nye vikoresh kwa mbizima uga ugrimunzi murugo harimo ibidibuga nivikoresho vjisuku bitonde tsenesa mutu vati dosizira njeri juu mwe eru tukwabi jenewe kuburidio abaguzi wabasha kuisanzura wakarebi vjo vacheneye bagashimu',\n",
       " 'mwadamu wamba ikanzu iga njumama waratandu kanya wamba ijitambaru uicha ikunebe mbere ya kawara meza nditse atiritsa nijikombe chungeru chamata ndiyo handelgu ya mwosogu ya kawaru ndi mga wamba ishati umukara nditse ninguofiru uigururu ndiyo maya kawari cha pachanditse wahanu harama ta meza inga',\n",
       " 'umuunu fashia mafuranga munuchi mafuranga tujirumaro mkuya haisha ibiriuga ibijokambara ndetanga wakawagara gazayuko usho rakuya vunjisha aho ujana mafuranga runaka gayushaka wakawaya guha',\n",
       " 'chapatis dekza nijikwembe cha mata omono ajie kubimwa kandi arabifata na breakfasti chapatis zirajio hachani awanya guandavenshi wakunda chapati',\n",
       " 'e modoc rupangu e camiu e ciuma e matar ce rama te ai ma doc e biraguri',\n",
       " 'churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churwa churwa chur',\n",
       " 'ichiapa chiri mwenyandiko yurulimiru kwa mahangu chandi chisigweho amagambo manini chane alijibu mibaradiju mngeru aliku aho alibi halimibaradiju mutuku hariko nimero shobora kwa magara ho kujirango bakuyobodi',\n",
       " 'ahangaha tulavani cha apa ninshuti nikowa vita arikowa nwa wa shingi kufata amafara anga ya uzanye ara maja mahanga baka guheriza mo amanya luguanda changu seo kaza na maja mahanga wa riyofite baka guheriza maja luguanda cheo kaza na maja luguanda baka guheriza maja mahanga bitewe na awa ujie kuja guhaira changu seo ujie kuja kuresho',\n",
       " 'uzo mkubwa nako iftivina juu kwa abidi mbingi mboga inyanya amashu tetseni mindi apanolo imaitanziza inyani misa tsefu unziru kwa sarana katerie ilinda jwa ajma rajenda',\n",
       " 'munu wambaye umupira umutuku ndetse ni taburia ijagusa umukara hirijaiwe harundi wambaye umupira ujagusungeru ndetse ya ambarie mwaga pira kajagusumukara imberiwe haramachupa abiri haruchupa jamavuta yubuto ndetse ni ndichupa risa umutuku',\n",
       " 'abashinzi kufuzi mamurgu anda bashi shikariza wa churuzi wa churuzi vijini mjerechi ya njina mata kujiri suku nitsewe kakuriri suku chane aha anu watunga njina mata bifukwa kwa amatia jimu mga anda ashubara kutiri nguara njizika vijie zilimona kuriri']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to 'test_predictions.csv'...\n",
      "✅ Results saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 6 (Optional): Save Results to a File ---\n",
    "# ===================================================================================\n",
    "print(\"Saving results to 'test_predictions.csv'...\")\n",
    "results_df = pd.DataFrame({\n",
    "    \"Reference\": references[:32],\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "results_df[\"wer\"] = results_df.apply(\n",
    "    lambda row: wer_metric.compute(predictions=[row.Prediction], references=[row.Reference]), axis=1\n",
    ")\n",
    "\n",
    "# results_df.to_csv(\"test_predictions_6k_steps__.csv\", index=False)\n",
    "print(\"✅ Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d8c6e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=\"wer\", ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5da8d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akajerekani gateretse hasi ku butaka gapfundik...</td>\n",
       "      <td>akajerekani ketereze aasi kutaka kepfundi tien...</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uburyo bwo kwishyura hakoreshejwe ikoranabuhan...</td>\n",
       "      <td>uwurijobu kweishira hakureishishge ikorana wuh...</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hano hari abantu batatu babiri muri bo ni ab'i...</td>\n",
       "      <td>hano hari awa nu batatu babiri muri vo na viji...</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abantu benshi bahagaze imbere y'inyubako ifite...</td>\n",
       "      <td>aba anu benshi wa gaze imbere iinyu baku fite ...</td>\n",
       "      <td>1.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahantu heza hafite isuku ndetse n'amacumbi was...</td>\n",
       "      <td>anoheza haftisukumdece na machombe wasoche rao...</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abantu batwara amapikipiki by'umwuga bahetse a...</td>\n",
       "      <td>aba anu watuara ama pichi pichi pichumunga wa ...</td>\n",
       "      <td>1.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abantu benshi cyane, bafite uruhu rwera, bamba...</td>\n",
       "      <td>awano vwenti chani wafite urvurguwela wambaye ...</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abagabo babiri bicaye ahantu hamwe, umwe yamba...</td>\n",
       "      <td>aba gawa wabilibicha ya hanu hamge umge ya amb...</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inshange ya emutiyeni irimo uburyo bwo kwishyu...</td>\n",
       "      <td>mshange ya mtn ilimu uulijewa kushira mafarang...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Waba ufite amafaranga y'amanyamahanga? Ni byiz...</td>\n",
       "      <td>mfita mafranga ya maja mahanga nibijiza kukana...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Umugabo wambaye umupira uri mu ibara ry'umukar...</td>\n",
       "      <td>mbuga wamba mwapiruri mni wararijumu karawandi...</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Isoko ricururizwamo ibicuruzwa bitandukanye ha...</td>\n",
       "      <td>kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hi...</td>\n",
       "      <td>4.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Benshi mu baturage bafite amafaranga ahagije b...</td>\n",
       "      <td>wenchimu waturajiwe nzifitea mufuranga ajiria ...</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Uburyo bwo kohererezanya amafaranga aturutse w...</td>\n",
       "      <td>uvurijo uko heririza nyama faraanga atutse uhe...</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mu nyubako nyinshi z'amahoteli ugenda uhasanga...</td>\n",
       "      <td>mwanyuba kujenshi za mahoteli ujendo hasanga i...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Inzu y'ubucuruzi irimo akabati gateretsemo iby...</td>\n",
       "      <td>nzui vuchuruzi idima kabati ga teletemo ibjoku...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Aho bavunjira amafaranga y'amanyamahanga, usho...</td>\n",
       "      <td>ahuwa bafunjira ama faranga ya manya mahanga u...</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Abantu babiri umwe akaba afashe telefone munto...</td>\n",
       "      <td>haba anuabiri umwe akawafasha telefone mwono c...</td>\n",
       "      <td>1.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ndabona muri butike harimo ibinyobwa by'amoko ...</td>\n",
       "      <td>ndiyogo namu mwari butike harimu divinyo mga y...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Amafaranga ari mu bwoko bw'inoti akaba ariho u...</td>\n",
       "      <td>amafranga arimu kukuginoti akawariho ururi mir...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Umugabo wambaye agapira k'umweru ndetse n'isha...</td>\n",
       "      <td>umukawa wambaya kapila kumweli nzete endisha h...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Icyumba gikorerwamo ubucuruzi bw'ibintu bitand...</td>\n",
       "      <td>echumba jikorelgo mo uvuchuruzi gibi nhu bitan...</td>\n",
       "      <td>1.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Umudamu wambaye ikanzu yiganjemo amabara atand...</td>\n",
       "      <td>mwadamu wamba ikanzu iga njumama waratandu kan...</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Umuntu ufashe amafaranga mu ntoki, amafaranga ...</td>\n",
       "      <td>umuunu fashia mafuranga munuchi mafuranga tuji...</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Capati ndetse n'igikombe cy'amata umuntu agiye...</td>\n",
       "      <td>chapatis dekza nijikwembe cha mata omono ajie ...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Imodoka, urupangu, ikamyo, icyuma, amatara, ik...</td>\n",
       "      <td>e modoc rupangu e camiu e ciuma e matar ce ram...</td>\n",
       "      <td>2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Inzu y'ubucuruzi icururizwamo ibinyobwa bisemb...</td>\n",
       "      <td>churru za churru za churru za churru za churru...</td>\n",
       "      <td>9.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Icyapa kiri munyandiko y'ururimi rw'amahanga, ...</td>\n",
       "      <td>ichiapa chiri mwenyandiko yurulimiru kwa mahan...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ahangaha turabona icyapa ni inshuti ni ko babi...</td>\n",
       "      <td>ahangaha tulavani cha apa ninshuti nikowa vita...</td>\n",
       "      <td>1.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Abazungu ubona ko bafite ibintu baje  guhaha b...</td>\n",
       "      <td>uzo mkubwa nako iftivina juu kwa abidi mbingi ...</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Umuntu wambaye umupira w'umutuku ndetse n'itab...</td>\n",
       "      <td>munu wambaye umupira umutuku ndetse ni taburia...</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Abashinzwe ubuzima mu Rwanda, bashishikariza a...</td>\n",
       "      <td>abashinzi kufuzi mamurgu anda bashi shikariza ...</td>\n",
       "      <td>1.258065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Reference  \\\n",
       "0   Akajerekani gateretse hasi ku butaka gapfundik...   \n",
       "1   Uburyo bwo kwishyura hakoreshejwe ikoranabuhan...   \n",
       "2   Hano hari abantu batatu babiri muri bo ni ab'i...   \n",
       "3   Abantu benshi bahagaze imbere y'inyubako ifite...   \n",
       "4   Ahantu heza hafite isuku ndetse n'amacumbi was...   \n",
       "5   Abantu batwara amapikipiki by'umwuga bahetse a...   \n",
       "6   Abantu benshi cyane, bafite uruhu rwera, bamba...   \n",
       "7   Abagabo babiri bicaye ahantu hamwe, umwe yamba...   \n",
       "8   Inshange ya emutiyeni irimo uburyo bwo kwishyu...   \n",
       "9   Waba ufite amafaranga y'amanyamahanga? Ni byiz...   \n",
       "10  Umugabo wambaye umupira uri mu ibara ry'umukar...   \n",
       "11  Isoko ricururizwamo ibicuruzwa bitandukanye ha...   \n",
       "12  Benshi mu baturage bafite amafaranga ahagije b...   \n",
       "13  Uburyo bwo kohererezanya amafaranga aturutse w...   \n",
       "14  Mu nyubako nyinshi z'amahoteli ugenda uhasanga...   \n",
       "15  Inzu y'ubucuruzi irimo akabati gateretsemo iby...   \n",
       "16  Aho bavunjira amafaranga y'amanyamahanga, usho...   \n",
       "17  Abantu babiri umwe akaba afashe telefone munto...   \n",
       "18  Ndabona muri butike harimo ibinyobwa by'amoko ...   \n",
       "19  Amafaranga ari mu bwoko bw'inoti akaba ariho u...   \n",
       "20  Umugabo wambaye agapira k'umweru ndetse n'isha...   \n",
       "21  Icyumba gikorerwamo ubucuruzi bw'ibintu bitand...   \n",
       "22  Umudamu wambaye ikanzu yiganjemo amabara atand...   \n",
       "23  Umuntu ufashe amafaranga mu ntoki, amafaranga ...   \n",
       "24  Capati ndetse n'igikombe cy'amata umuntu agiye...   \n",
       "25  Imodoka, urupangu, ikamyo, icyuma, amatara, ik...   \n",
       "26  Inzu y'ubucuruzi icururizwamo ibinyobwa bisemb...   \n",
       "27  Icyapa kiri munyandiko y'ururimi rw'amahanga, ...   \n",
       "28  Ahangaha turabona icyapa ni inshuti ni ko babi...   \n",
       "29  Abazungu ubona ko bafite ibintu baje  guhaha b...   \n",
       "30  Umuntu wambaye umupira w'umutuku ndetse n'itab...   \n",
       "31  Abashinzwe ubuzima mu Rwanda, bashishikariza a...   \n",
       "\n",
       "                                           Prediction       wer  \n",
       "0   akajerekani ketereze aasi kutaka kepfundi tien...  1.125000  \n",
       "1   uwurijobu kweishira hakureishishge ikorana wuh...  1.250000  \n",
       "2   hano hari awa nu batatu babiri muri vo na viji...  0.793103  \n",
       "3   aba anu benshi wa gaze imbere iinyu baku fite ...  1.292683  \n",
       "4   anoheza haftisukumdece na machombe wasoche rao...  0.944444  \n",
       "5   aba anu watuara ama pichi pichi pichumunga wa ...  1.421053  \n",
       "6   awano vwenti chani wafite urvurguwela wambaye ...  0.954545  \n",
       "7   aba gawa wabilibicha ya hanu hamge umge ya amb...  0.964286  \n",
       "8   mshange ya mtn ilimu uulijewa kushira mafarang...  1.000000  \n",
       "9   mfita mafranga ya maja mahanga nibijiza kukana...  1.000000  \n",
       "10  mbuga wamba mwapiruri mni wararijumu karawandi...  0.957447  \n",
       "11  kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hi...  4.484848  \n",
       "12  wenchimu waturajiwe nzifitea mufuranga ajiria ...  0.966667  \n",
       "13  uvurijo uko heririza nyama faraanga atutse uhe...  1.071429  \n",
       "14  mwanyuba kujenshi za mahoteli ujendo hasanga i...  1.000000  \n",
       "15  nzui vuchuruzi idima kabati ga teletemo ibjoku...  1.000000  \n",
       "16  ahuwa bafunjira ama faranga ya manya mahanga u...  1.333333  \n",
       "17  haba anuabiri umwe akawafasha telefone mwono c...  1.035714  \n",
       "18  ndiyogo namu mwari butike harimu divinyo mga y...  1.000000  \n",
       "19  amafranga arimu kukuginoti akawariho ururi mir...  1.000000  \n",
       "20  umukawa wambaya kapila kumweli nzete endisha h...  1.000000  \n",
       "21  echumba jikorelgo mo uvuchuruzi gibi nhu bitan...  1.029412  \n",
       "22  mwadamu wamba ikanzu iga njumama waratandu kan...  0.953488  \n",
       "23  umuunu fashia mafuranga munuchi mafuranga tuji...  0.962963  \n",
       "24  chapatis dekza nijikwembe cha mata omono ajie ...  0.950000  \n",
       "25  e modoc rupangu e camiu e ciuma e matar ce ram...  2.125000  \n",
       "26  churru za churru za churru za churru za churru...  9.208333  \n",
       "27  ichiapa chiri mwenyandiko yurulimiru kwa mahan...  1.000000  \n",
       "28  ahangaha tulavani cha apa ninshuti nikowa vita...  1.285714  \n",
       "29  uzo mkubwa nako iftivina juu kwa abidi mbingi ...  1.040000  \n",
       "30  munu wambaye umupira umutuku ndetse ni taburia...  0.789474  \n",
       "31  abashinzi kufuzi mamurgu anda bashi shikariza ...  1.258065  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7157d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akajerekani gateretse hasi ku butaka gapfundik...</td>\n",
       "      <td>akajerekani ketereze aasi kutaka kepfundi tien...</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uburyo bwo kwishyura hakoreshejwe ikoranabuhan...</td>\n",
       "      <td>uwurijobu kweishira hakureishishge ikorana wuh...</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hano hari abantu batatu babiri muri bo ni ab'i...</td>\n",
       "      <td>hano hari awa nu batatu babiri muri vo na viji...</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abantu benshi bahagaze imbere y'inyubako ifite...</td>\n",
       "      <td>aba anu benshi wa gaze imbere iinyu baku fite ...</td>\n",
       "      <td>1.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahantu heza hafite isuku ndetse n'amacumbi was...</td>\n",
       "      <td>anoheza haftisukumdece na machombe wasoche rao...</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abantu batwara amapikipiki by'umwuga bahetse a...</td>\n",
       "      <td>aba anu watuara ama pichi pichi pichumunga wa ...</td>\n",
       "      <td>1.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abantu benshi cyane, bafite uruhu rwera, bamba...</td>\n",
       "      <td>awano vwenti chani wafite urvurguwela wambaye ...</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abagabo babiri bicaye ahantu hamwe, umwe yamba...</td>\n",
       "      <td>aba gawa wabilibicha ya hanu hamge umge ya amb...</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inshange ya emutiyeni irimo uburyo bwo kwishyu...</td>\n",
       "      <td>mshange ya mtn ilimu uulijewa kushira mafarang...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Waba ufite amafaranga y'amanyamahanga? Ni byiz...</td>\n",
       "      <td>mfita mafranga ya maja mahanga nibijiza kukana...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Umugabo wambaye umupira uri mu ibara ry'umukar...</td>\n",
       "      <td>mbuga wamba mwapiruri mni wararijumu karawandi...</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Isoko ricururizwamo ibicuruzwa bitandukanye ha...</td>\n",
       "      <td>kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hi...</td>\n",
       "      <td>4.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Benshi mu baturage bafite amafaranga ahagije b...</td>\n",
       "      <td>wenchimu waturajiwe nzifitea mufuranga ajiria ...</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Uburyo bwo kohererezanya amafaranga aturutse w...</td>\n",
       "      <td>uvurijo uko heririza nyama faraanga atutse uhe...</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mu nyubako nyinshi z'amahoteli ugenda uhasanga...</td>\n",
       "      <td>mwanyuba kujenshi za mahoteli ujendo hasanga i...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Inzu y'ubucuruzi irimo akabati gateretsemo iby...</td>\n",
       "      <td>nzui vuchuruzi idima kabati ga teletemo ibjoku...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Aho bavunjira amafaranga y'amanyamahanga, usho...</td>\n",
       "      <td>ahuwa bafunjira ama faranga ya manya mahanga u...</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Abantu babiri umwe akaba afashe telefone munto...</td>\n",
       "      <td>haba anuabiri umwe akawafasha telefone mwono c...</td>\n",
       "      <td>1.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ndabona muri butike harimo ibinyobwa by'amoko ...</td>\n",
       "      <td>ndiyogo namu mwari butike harimu divinyo mga y...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Amafaranga ari mu bwoko bw'inoti akaba ariho u...</td>\n",
       "      <td>amafranga arimu kukuginoti akawariho ururi mir...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Umugabo wambaye agapira k'umweru ndetse n'isha...</td>\n",
       "      <td>umukawa wambaya kapila kumweli nzete endisha h...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Icyumba gikorerwamo ubucuruzi bw'ibintu bitand...</td>\n",
       "      <td>echumba jikorelgo mo uvuchuruzi gibi nhu bitan...</td>\n",
       "      <td>1.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Umudamu wambaye ikanzu yiganjemo amabara atand...</td>\n",
       "      <td>mwadamu wamba ikanzu iga njumama waratandu kan...</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Umuntu ufashe amafaranga mu ntoki, amafaranga ...</td>\n",
       "      <td>umuunu fashia mafuranga munuchi mafuranga tuji...</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Capati ndetse n'igikombe cy'amata umuntu agiye...</td>\n",
       "      <td>chapatis dekza nijikwembe cha mata omono ajie ...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Imodoka, urupangu, ikamyo, icyuma, amatara, ik...</td>\n",
       "      <td>e modoc rupangu e camiu e ciuma e matar ce ram...</td>\n",
       "      <td>2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Inzu y'ubucuruzi icururizwamo ibinyobwa bisemb...</td>\n",
       "      <td>churru za churru za churru za churru za churru...</td>\n",
       "      <td>9.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Icyapa kiri munyandiko y'ururimi rw'amahanga, ...</td>\n",
       "      <td>ichiapa chiri mwenyandiko yurulimiru kwa mahan...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ahangaha turabona icyapa ni inshuti ni ko babi...</td>\n",
       "      <td>ahangaha tulavani cha apa ninshuti nikowa vita...</td>\n",
       "      <td>1.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Abazungu ubona ko bafite ibintu baje  guhaha b...</td>\n",
       "      <td>uzo mkubwa nako iftivina juu kwa abidi mbingi ...</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Umuntu wambaye umupira w'umutuku ndetse n'itab...</td>\n",
       "      <td>munu wambaye umupira umutuku ndetse ni taburia...</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Abashinzwe ubuzima mu Rwanda, bashishikariza a...</td>\n",
       "      <td>abashinzi kufuzi mamurgu anda bashi shikariza ...</td>\n",
       "      <td>1.258065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Reference  \\\n",
       "0   Akajerekani gateretse hasi ku butaka gapfundik...   \n",
       "1   Uburyo bwo kwishyura hakoreshejwe ikoranabuhan...   \n",
       "2   Hano hari abantu batatu babiri muri bo ni ab'i...   \n",
       "3   Abantu benshi bahagaze imbere y'inyubako ifite...   \n",
       "4   Ahantu heza hafite isuku ndetse n'amacumbi was...   \n",
       "5   Abantu batwara amapikipiki by'umwuga bahetse a...   \n",
       "6   Abantu benshi cyane, bafite uruhu rwera, bamba...   \n",
       "7   Abagabo babiri bicaye ahantu hamwe, umwe yamba...   \n",
       "8   Inshange ya emutiyeni irimo uburyo bwo kwishyu...   \n",
       "9   Waba ufite amafaranga y'amanyamahanga? Ni byiz...   \n",
       "10  Umugabo wambaye umupira uri mu ibara ry'umukar...   \n",
       "11  Isoko ricururizwamo ibicuruzwa bitandukanye ha...   \n",
       "12  Benshi mu baturage bafite amafaranga ahagije b...   \n",
       "13  Uburyo bwo kohererezanya amafaranga aturutse w...   \n",
       "14  Mu nyubako nyinshi z'amahoteli ugenda uhasanga...   \n",
       "15  Inzu y'ubucuruzi irimo akabati gateretsemo iby...   \n",
       "16  Aho bavunjira amafaranga y'amanyamahanga, usho...   \n",
       "17  Abantu babiri umwe akaba afashe telefone munto...   \n",
       "18  Ndabona muri butike harimo ibinyobwa by'amoko ...   \n",
       "19  Amafaranga ari mu bwoko bw'inoti akaba ariho u...   \n",
       "20  Umugabo wambaye agapira k'umweru ndetse n'isha...   \n",
       "21  Icyumba gikorerwamo ubucuruzi bw'ibintu bitand...   \n",
       "22  Umudamu wambaye ikanzu yiganjemo amabara atand...   \n",
       "23  Umuntu ufashe amafaranga mu ntoki, amafaranga ...   \n",
       "24  Capati ndetse n'igikombe cy'amata umuntu agiye...   \n",
       "25  Imodoka, urupangu, ikamyo, icyuma, amatara, ik...   \n",
       "26  Inzu y'ubucuruzi icururizwamo ibinyobwa bisemb...   \n",
       "27  Icyapa kiri munyandiko y'ururimi rw'amahanga, ...   \n",
       "28  Ahangaha turabona icyapa ni inshuti ni ko babi...   \n",
       "29  Abazungu ubona ko bafite ibintu baje  guhaha b...   \n",
       "30  Umuntu wambaye umupira w'umutuku ndetse n'itab...   \n",
       "31  Abashinzwe ubuzima mu Rwanda, bashishikariza a...   \n",
       "\n",
       "                                           Prediction       wer  \n",
       "0   akajerekani ketereze aasi kutaka kepfundi tien...  1.125000  \n",
       "1   uwurijobu kweishira hakureishishge ikorana wuh...  1.250000  \n",
       "2   hano hari awa nu batatu babiri muri vo na viji...  0.793103  \n",
       "3   aba anu benshi wa gaze imbere iinyu baku fite ...  1.292683  \n",
       "4   anoheza haftisukumdece na machombe wasoche rao...  0.944444  \n",
       "5   aba anu watuara ama pichi pichi pichumunga wa ...  1.421053  \n",
       "6   awano vwenti chani wafite urvurguwela wambaye ...  0.954545  \n",
       "7   aba gawa wabilibicha ya hanu hamge umge ya amb...  0.964286  \n",
       "8   mshange ya mtn ilimu uulijewa kushira mafarang...  1.000000  \n",
       "9   mfita mafranga ya maja mahanga nibijiza kukana...  1.000000  \n",
       "10  mbuga wamba mwapiruri mni wararijumu karawandi...  0.957447  \n",
       "11  kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hi...  4.484848  \n",
       "12  wenchimu waturajiwe nzifitea mufuranga ajiria ...  0.966667  \n",
       "13  uvurijo uko heririza nyama faraanga atutse uhe...  1.071429  \n",
       "14  mwanyuba kujenshi za mahoteli ujendo hasanga i...  1.000000  \n",
       "15  nzui vuchuruzi idima kabati ga teletemo ibjoku...  1.000000  \n",
       "16  ahuwa bafunjira ama faranga ya manya mahanga u...  1.333333  \n",
       "17  haba anuabiri umwe akawafasha telefone mwono c...  1.035714  \n",
       "18  ndiyogo namu mwari butike harimu divinyo mga y...  1.000000  \n",
       "19  amafranga arimu kukuginoti akawariho ururi mir...  1.000000  \n",
       "20  umukawa wambaya kapila kumweli nzete endisha h...  1.000000  \n",
       "21  echumba jikorelgo mo uvuchuruzi gibi nhu bitan...  1.029412  \n",
       "22  mwadamu wamba ikanzu iga njumama waratandu kan...  0.953488  \n",
       "23  umuunu fashia mafuranga munuchi mafuranga tuji...  0.962963  \n",
       "24  chapatis dekza nijikwembe cha mata omono ajie ...  0.950000  \n",
       "25  e modoc rupangu e camiu e ciuma e matar ce ram...  2.125000  \n",
       "26  churru za churru za churru za churru za churru...  9.208333  \n",
       "27  ichiapa chiri mwenyandiko yurulimiru kwa mahan...  1.000000  \n",
       "28  ahangaha tulavani cha apa ninshuti nikowa vita...  1.285714  \n",
       "29  uzo mkubwa nako iftivina juu kwa abidi mbingi ...  1.040000  \n",
       "30  munu wambaye umupira umutuku ndetse ni taburia...  0.789474  \n",
       "31  abashinzi kufuzi mamurgu anda bashi shikariza ...  1.258065  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
