{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8447bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334c0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 1: Configuration - UPDATE THESE PATHS ---\n",
    "# ===================================================================================\n",
    "# The original model you fine-tuned\n",
    "MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "\n",
    "# Path to the LoRA adapter you trained and saved\n",
    "# This is the directory created by `trainer.save_model()`\n",
    "ADAPTER_PATH = \"/ocean/projects/cis250085p/shared/A_track/whisper-large-v3-lora-streaming/checkpoint-4000\"\n",
    "\n",
    "# Path to your test data metadata file\n",
    "TEST_DATA_CSV = \"/ocean/projects/cis250085p/shared/A_track/dev_test.json\" # <-- IMPORTANT: Change this!\n",
    "\n",
    "# The base path where your raw audio files are stored, same as in training\n",
    "RAW_AUDIO_BASE_PATH = \"/ocean/projects/cis250085p/shared/A_track/\"\n",
    "\n",
    "# Configuration for the prediction run\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 2 # Adjust based on your GPU memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "880e69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load bitsandbytes native library: /lib64/libc.so.6: version `GLIBC_2.34' not found (required by /ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/ctypes/__init__.py\", line 454, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: /lib64/libc.so.6: version `GLIBC_2.34' not found (required by /ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda126.so)\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and configured for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.encoder.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.6.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.7.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.8.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.9.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.10.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.11.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.12.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.13.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.14.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.15.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.16.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.17.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.18.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.19.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.20.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.21.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.22.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.23.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.24.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.25.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.26.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.27.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.28.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.29.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.30.encoder_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.v_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.v_proj.lora_B.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.q_proj.lora_A.default.weight', 'base_model.model.model.decoder.layers.31.encoder_attn.q_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 2: Load Your Fine-Tuned Model and Processor ---\n",
    "# ===================================================================================\n",
    "print(\"Loading model and processor...\")\n",
    "\n",
    "# Load the processor\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "processor.tokenizer.set_prefix_tokens(language=\"kinyarwanda\", task=\"transcribe\")\n",
    "\n",
    "# Load the base model in float16 for faster inference\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME, torch_dtype=torch.float16,\n",
    "    cache_dir = \"/ocean/projects/cis250085p/shared/A_track\"\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "# # # model = PeftModel.from_pretrained(base_model, ADAPTER_PATH).to(DEVICE)\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH, is_trainable=True)\n",
    "\n",
    "# model.enable_adapter() \n",
    "\n",
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "base_model.eval()\n",
    "# --- OPTIONAL BUT RECOMMENDED: Apply optimizations from training ---\n",
    "# 1. Use greedy search for maximum speed\n",
    "# model.generation_config.num_beams = 1\n",
    "# model.generation_config.do_sample = False\n",
    "# 2. Compile the model if using PyTorch 2.0+\n",
    "# model = torch.compile(model)\n",
    "\n",
    "print(\"✅ Model loaded and configured for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f7dd7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ADAPTER_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msafetensors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_file \n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Load the saved weights into memory\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m saved_adapter_weights = torch.load( \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mADAPTER_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/adapter_model.bin\u001b[39m\u001b[33m\"\u001b[39m, map_location=\u001b[33m\"\u001b[39m\u001b[33mGPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create a new dictionary to hold the corrected keys\u001b[39;00m\n\u001b[32m      6\u001b[39m new_state_dict = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'ADAPTER_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file \n",
    "# Load the saved weights into memory\n",
    "saved_adapter_weights = torch.load( f\"{ADAPTER_PATH}/adapter_model.bin\", map_location=\"GPU\")\n",
    "\n",
    "# Create a new dictionary to hold the corrected keys\n",
    "new_state_dict = {}\n",
    "# Loop through the saved keys and rename them to match the current model structure\n",
    "for key, value in saved_adapter_weights.items():\n",
    "    # This replaces the erroneous '...model.model...' with the correct '...model...'\n",
    "    new_key = key.replace(\"base_model.model.model.\", \"base_model.model.\", 1)\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "# Load our corrected weights into the model.\n",
    "# `strict=False` ignores any non-matching keys, which is what we want.\n",
    "print(\"Loading corrected weights into the model...\")\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "\n",
    "# --- 4. Finalize the Model for Training/Inference ---\n",
    "model.eval() # Or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf71df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: ./A_track/whisper-large-v3-lora-streaming/checkpoint-4000/adapter_model.safetensors\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xec'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 7\u001b[39m\n",
      "\u001b[32m      4\u001b[39m adapter_weights_path = \u001b[33m\"\u001b[39m\u001b[33m./A_track/whisper-large-v3-lora-streaming/checkpoint-4000/adapter_model.safetensors\u001b[39m\u001b[33m\"\u001b[39m \n",
      "\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading weights from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madapter_weights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m adapter_weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_weights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Let's inspect a key from one of the deeper layers in the decoder\u001b[39;00m\n",
      "\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# This key should have been trained and have non-zero values\u001b[39;00m\n",
      "\u001b[32m     11\u001b[39m key_to_check = \u001b[33m\"\u001b[39m\u001b[33mbase_model.model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/serialization.py:1549\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n",
      "\u001b[32m   1547\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[32m   1548\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\n",
      "\u001b[32m   1551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/serialization.py:1797\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n",
      "\u001b[32m   1794\u001b[39m         \u001b[38;5;66;03m# if not a tarfile, reset file offset and proceed\u001b[39;00m\n",
      "\u001b[32m   1795\u001b[39m         f.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m magic_number = \u001b[43mpickle_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic_number != MAGIC_NUMBER:\n",
      "\u001b[32m   1799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid magic number; corrupt file?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mUnpicklingError\u001b[39m: invalid load key, '\\xec'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your saved adapter weights file\n",
    "adapter_weights_path = \"./A_track/whisper-large-v3-lora-streaming/checkpoint-4000/adapter_model.safetensors\" \n",
    "\n",
    "print(f\"Loading weights from: {adapter_weights_path}\")\n",
    "adapter_weights = torch.load(adapter_weights_path, weights_only=False)\n",
    "\n",
    "# Let's inspect a key from one of the deeper layers in the decoder\n",
    "# This key should have been trained and have non-zero values\n",
    "key_to_check = \"base_model.model.model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight\"\n",
    "\n",
    "if key_to_check in adapter_weights:\n",
    "    weights = adapter_weights[key_to_check]\n",
    "    print(f\"\\nSuccessfully found key: {key_to_check}\")\n",
    "    print(\"Shape of weights tensor:\", weights.shape)\n",
    "    print(\"A small sample of the weights:\")\n",
    "    print(weights)\n",
    "    print(f\"\\nMean of absolute values: {weights.abs().float().mean()}\")\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR: Could not find the key '{key_to_check}' in the adapter file.\")\n",
    "    print(\"This indicates a serious problem with the saved checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de62a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis250085p/shared\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88283bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_df= pd.read_json(TEST_DATA_CSV).T\n",
    "# TD_df[\"file_path\"] = \"processed/\"+ TD_df[\"audio_path\"] +\".mel.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd47ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_df[\"audio_path\"] = TD_df.audio_path.str.replace(\"audio/\",\"/ocean/projects/cis250085p/shared/track_a_audio_files/\")  +\".wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3fe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/ocean/projects/cis250085p/shared/track_a_audio_files/1739532284-OogTF7X5UsTPNsR9q4GLZYcJiKB2.wav'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TD_df.audio_path.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1eabed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from: /ocean/projects/cis250085p/shared/A_track/dev_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4632/4632 [06:42<00:00, 11.52 examples/s]\n",
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4632/4632 [13:30<00:00,  5.72 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test dataset prepared with 4632 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 3: Load and Prepare the Test Dataset ---\n",
    "# ===================================================================================\n",
    "print(f\"Loading test dataset from: {TEST_DATA_CSV}\")\n",
    "\n",
    "# Load the metadata\n",
    "test_dataset = Dataset.from_pandas(TD_df)\n",
    "\n",
    "# Function to prepare a single example for the model\n",
    "# It loads the audio and converts it into the 'input_features' the model expects\n",
    "def prepare_dataset(example):\n",
    "    audio_path = example[\"audio_path\"]\n",
    "    try:\n",
    "        # --- THIS IS THE NEW, MORE ROBUST METHOD ---\n",
    "        # 1. Load audio directly with torchaudio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # 2. The Whisper processor will handle resampling to 16kHz automatically\n",
    "        # We pass the raw waveform and its original sample rate\n",
    "        input_features = processor(\n",
    "            waveform.squeeze(), # Remove channel dimension\n",
    "            sampling_rate=sample_rate\n",
    "        ).input_features[0]\n",
    "        # --- END OF NEW METHOD ---\n",
    "\n",
    "        # 3. Convert to float16 to match the model\n",
    "        example[\"input_features\"] = torch.from_numpy(input_features).to(torch.float16)\n",
    "\n",
    "    except Exception as e:\n",
    "        # This will now catch any errors from torchaudio or the processor\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        example[\"input_features\"] = None # Mark as None to filter later\n",
    "        \n",
    "    return example\n",
    "\n",
    "# Apply the preparation function\n",
    "test_dataset = test_dataset.map(prepare_dataset, num_proc=4)\n",
    "# Filter out any samples that failed to load\n",
    "# test_dataset = test_dataset.filter(lambda example: example[\"input_features\"] is not None)\n",
    "\n",
    "print(f\"✅ Test dataset prepared with {len(test_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6adb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                          | 0/290 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                                                                                               | 2/290 [00:45<1:48:36, 22.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 24\u001b[39m\n",
      "\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run prediction\u001b[39;00m\n",
      "\u001b[32m     23\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \u001b[38;5;66;03m# Disable gradient calculation for inference\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     predicted_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Decode the predicted IDs to text\u001b[39;00m\n",
      "\u001b[32m     27\u001b[39m transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/peft/peft_model.py:823\u001b[39m, in \u001b[36mPeftModel.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m    821\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n",
      "\u001b[32m    822\u001b[39m     kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n",
      "\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:774\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate\u001b[39m\u001b[34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, time_precision_features, return_token_timestamps, return_segments, return_dict_in_generate, force_unique_generate_call, **kwargs)\u001b[39m\n",
      "\u001b[32m    765\u001b[39m             proc.set_begin_index(decoder_input_ids.shape[-\u001b[32m1\u001b[39m])\n",
      "\u001b[32m    767\u001b[39m \u001b[38;5;66;03m# 6.6 Run generate with fallback\u001b[39;00m\n",
      "\u001b[32m    768\u001b[39m (\n",
      "\u001b[32m    769\u001b[39m     seek_sequences,\n",
      "\u001b[32m    770\u001b[39m     seek_outputs,\n",
      "\u001b[32m    771\u001b[39m     should_skip,\n",
      "\u001b[32m    772\u001b[39m     do_condition_on_prev_tokens,\n",
      "\u001b[32m    773\u001b[39m     model_output_type,\n",
      "\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_fallback\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcur_bsz\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_idx_map\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseek\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseek\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperatures\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_timestamps\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_condition_on_prev_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_shortform\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    794\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    796\u001b[39m \u001b[38;5;66;03m# 6.7 In every generated sequence, split by timestamp tokens and extract segments\u001b[39;00m\n",
      "\u001b[32m    797\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seek_sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seek_sequences):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:950\u001b[39m, in \u001b[36mWhisperGenerationMixin.generate_with_fallback\u001b[39m\u001b[34m(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, is_shortform, batch_size, attention_mask, kwargs)\u001b[39m\n",
      "\u001b[32m    945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m generate_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    946\u001b[39m         generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m] = F.pad(\n",
      "\u001b[32m    947\u001b[39m             generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mencoder_outputs\u001b[39m\u001b[33m\"\u001b[39m], (\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, batch_size - cur_bsz), value=\u001b[32m0\u001b[39m\n",
      "\u001b[32m    948\u001b[39m         )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m seek_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43msegment_input\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    962\u001b[39m model_output_type = \u001b[38;5;28mtype\u001b[39m(seek_outputs)\n",
      "\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# post-process sequence tokens and outputs to be in list form\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n",
      "\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n",
      "\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n",
      "\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n",
      "\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n",
      "\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n",
      "\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n",
      "\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n",
      "\u001b[32m   2593\u001b[39m         **model_kwargs,\n",
      "\u001b[32m   2594\u001b[39m     )\n",
      "\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n",
      "\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n",
      "\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n",
      "\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n",
      "\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n",
      "\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n",
      "\u001b[32m   2613\u001b[39m         **model_kwargs,\n",
      "\u001b[32m   2614\u001b[39m     )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n",
      "\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n",
      "\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n",
      "\u001b[32m   3564\u001b[39m     outputs,\n",
      "\u001b[32m   3565\u001b[39m     model_kwargs,\n",
      "\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n",
      "\u001b[32m   3567\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1694\u001b[39m, in \u001b[36mWhisperForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n",
      "\u001b[32m   1689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m   1690\u001b[39m         decoder_input_ids = shift_tokens_right(\n",
      "\u001b[32m   1691\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n",
      "\u001b[32m   1692\u001b[39m         )\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1695\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1712\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.proj_out(outputs[\u001b[32m0\u001b[39m])\n",
      "\u001b[32m   1714\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1529\u001b[39m, in \u001b[36mWhisperModel.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n",
      "\u001b[32m   1522\u001b[39m     encoder_outputs = BaseModelOutput(\n",
      "\u001b[32m   1523\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n",
      "\u001b[32m   1524\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[32m   1525\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[32m   1526\u001b[39m     )\n",
      "\u001b[32m   1528\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/transformers/models/whisper/modeling_whisper.py:1188\u001b[39m, in \u001b[36mWhisperDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n",
      "\u001b[32m   1174\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n",
      "\u001b[32m   1175\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n",
      "\u001b[32m   1176\u001b[39m         hidden_states,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1185\u001b[39m         cache_position,\n",
      "\u001b[32m   1186\u001b[39m     )\n",
      "\u001b[32m   1187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1201\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[32m   1203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis250085p/shared/envPreproces/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n",
      "\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n",
      "\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 4: Run the Prediction Loop ---\n",
    "# ===================================================================================\n",
    "# The data collator just needs to organize the batch\n",
    "def collate_fn(features):\n",
    "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "    batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "    return batch\n",
    "\n",
    "# Create a DataLoader for efficient batching\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "predictions = []\n",
    "references = test_dataset[\"transcription\"] # Get all ground truth transcriptions\n",
    "\n",
    "print(\"Running predictions on the test set...\")\n",
    "# Loop through the test data with a progress bar\n",
    "for batch in tqdm(test_dataloader):\n",
    "    # Move batch to the GPU\n",
    "    inputs = batch[\"input_features\"].to(DEVICE)\n",
    "\n",
    "    # Run prediction\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        predicted_ids = model.generate(inputs)\n",
    "\n",
    "    # Decode the predicted IDs to text\n",
    "    transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=True, normalize=True)\n",
    "    predictions.extend(transcriptions)\n",
    "\n",
    "print(\"✅ Prediction loop complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c1c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Akajerekani gateretse hasi ku butaka gapfundikiye neza, gapfundiki umuvuniko w'umweru n'akajerekani karasa n'umweru, akajerekani karimo amata.\",\n",
       " 'Uburyo bwo kwishyura hakoreshejwe ikoranabuhanga, kubitsa, kubikuza, kohereza amafaranga, ukoresheje terefone ngendanwa.',\n",
       " \"Hano hari abantu batatu babiri muri bo ni ab'igitsina gabo, undi umwe usigaye ni uw'igitsina gore, bicaye ku ntebe z'ibara ry'umweru ndetse imbere yabo bari kunywa amata mu birahure.\",\n",
       " \"Abantu benshi bahagaze imbere y'inyubako ifite amarangi y'umutuku n'umweru, yanditseho amagambo ari mu rurimi rw'icyongereza ari mu ibara ry'umutuku, abagabo barimo bambaye imyenda itandukanye, hari bambaye amashati y'umweru ndetse n'amapantaro y'umukara, hari n'abambaye amakote na karavate, hari n'uwambaye ingofero na rinete.\",\n",
       " \"Ahantu heza hafite isuku ndetse n'amacumbi wasohokera ukaruhuka, hari amazi meza akorerwamo imyidagaduro ndetse na siporo ziruhura imiruho.\",\n",
       " \"Abantu batwara amapikipiki by'umwuga bahetse abagore ku mapikipiki yabo, bose bahagaze iruhande bahindukiye bari kuvugisha abo bahetse ku mapikipiki.\",\n",
       " \"Abantu benshi cyane, bafite uruhu rwera, bambaye imyenda itandukanye, harimo umwe wicaye ndetse n'abandi bahagaze, bari kureba ku meza, iteretseho ibintu bitandukanye...\",\n",
       " \"Abagabo babiri bicaye ahantu hamwe, umwe yambaye ishati y'umweru imbere ye hari mudasobwa, undi yambaye ishati irimo amabara atandukanye n'ipantaro y'umukara ndetse afite agakombe mu ntoki ari kunywa.\",\n",
       " \"Inshange ya emutiyeni irimo uburyo bwo kwishyura amafaranga serivisi za emutiyeni kohereza amafaranga n'ukuntu amafaranga yoherezwa mu Bugande.\",\n",
       " \"Waba ufite amafaranga y'amanyamahanga? Ni byiza kugana forex bureau bakaguhamo amanyarwanda, cyangwa se niba umuntu yayakohereje kuri telefone wo hanze ni byiza kugana Western union bakayakubikurira.\",\n",
       " \"Umugabo wambaye umupira uri mu ibara ry'umukara, wanditseho polise mu ibara ry'umweru, uhagaze ku kazu ka emutiyene kari mu ibara ry'umuhondo, gatanga serivise zitandukanye, yaba kubitsa, kohereza, kugura amayinitse, ndetse no kugurisha amayinite. Mu kuboko kw'iburyo kwe hakaba haparitsemo cyangwa se hari amakarito ari mu ibara ry'ubururu.\",\n",
       " \"Isoko ricururizwamo ibicuruzwa bitandukanye harimo ibihingwa, imyaka nk'ibishyimbo, ibigori, ingano, ndetse n'ibindi biribwa nk'amasaka, ibitoki ndetse n'ibigori, iri soko rifite n'utundi ducururizwamo, ibikoresho byo mu rugo nk'amabase n'imikubuzo ndetse n'ibikoresho byo mu rugo.\",\n",
       " 'Benshi mu baturage bafite amafaranga ahagije bakunze kujya mu mabanki cyangwa ku mirongo igiye itandukanye bakoresha mu guhamagara cyangwa kugura interineti bagiye gufata inguzanyo kugira ngo bizinesi zabo zitere imbere.',\n",
       " 'Uburyo bwo kohererezanya amafaranga aturutse wohereza mumahanga cyangwa se wohererezwa ukoresheje uburyo bwa manipura.',\n",
       " \"Mu nyubako nyinshi z'amahoteli ugenda uhasanga ibikorwa byinshi bigiye bitandukanye bigufasha kuruhuka aho ushobora kuhasanga nk'ubwogero ushobora kujya mu mazi ukaba waruhukiramo ndetse ukahasanga naho wakwicara hirengeye kandi heza hatekanye.\",\n",
       " \"Inzu y'ubucuruzi irimo akabati gateretsemo ibyo kunywa bitandukanye, ji y'Inyange, coca n'imyembe na za enaji, harimo n'icyuma gikusanya amata na teremusi n'indobo iteretsemo amandazi n'umukobwa wicaye ku ntebe n'umugore wicaye ku ntebe arimo kunywa.\",\n",
       " \"Aho bavunjira amafaranga y'amanyamahanga, ushobora kuzana amadolari bakakuvunjira bakaguha amanyarwanda ndetse wanaba ushaka amadolari ufite amanyarwanda nabwo bakakuvunjira bakaguha agaciro kayo.\",\n",
       " 'Abantu babiri umwe akaba afashe telefone muntoki, bakaba bari kwerekana uburyo bushobora kwifashishwa hakoreshejwe ikoranabuhanga, hanyuma umuntu akaba yakwishyura ibicuruzwa bitandukanye, cyangwa se akabasha kohereza amafaranga no kuyakira.',\n",
       " \"Ndabona muri butike harimo ibinyobwa by'amoko menshi biri muri firigo, aho biba bikonjeshwa akenshi bino ni n'ibyo mu nganda.\",\n",
       " \"Amafaranga ari mu bwoko bw'inoti akaba ariho ururimi rw'igishinwa ndetse n'umugabo umenyerewe muri icyo gihugu, akaba ariho imibare igenda igaragaza agaciro kayo, ndetse n'uburyo akoreshwa.\",\n",
       " \"Umugabo wambaye agapira k'umweru ndetse n'ishati y'amaboko maremare,  akaba ari kumwe n'umugore wambaye umupira w'amaboko maremare bombi bakabamo baraseka, uyu mugore afite urufunguzo mu ntoki,  bukaba ari ubutumwa bw'ikigo gitanga inguzanyo kikubwira ko wabona ubu ngubu inzu yawe ukagenda wishyura gake gake.\",\n",
       " \"Icyumba gikorerwamo ubucuruzi bw'ibintu bitandukanye bikoreshwa mu buzima bwa buri munsi mu rugo, harimo ibiribwa n'ibikoresho by'isuku bitondetse neza mu tubati dusize irangi ry'umweru twabigenewe ku buryo abaguzi babasha kwisanzura bakareba ibyo bakeneye bagashima. \",\n",
       " 'Umudamu wambaye ikanzu yiganjemo amabara atandukanye, wambaye igitambaro, wicaye ku ntebe, imbere ye hakaba hari ameza ndetse ateretse n\\'igikombe cy\\'umweru cy\\'amata. Iruhande rw\\'ibumoso hakaba hari undi mugabo wambaye ishati y\\'umukara ndetse n\\'ingofero y\\'ubururu, inyuma hakaba hari icyapa cyanditseho \"hano hari amata meza y\\'inka\". ',\n",
       " 'Umuntu ufashe amafaranga mu ntoki, amafaranga atagirira umumaro mu kuyahahisha ibiribwa, ibyo kwambara, ndetse bakaba bagaragaza yuko ushobora kuyavunjisha, aho ujyana amafaranga runaka, ayo ushaka bakaba bayaguha.',\n",
       " \"Capati ndetse n'igikombe cy'amata umuntu agiye kubinywa kandi arabifata nka bureke fasiti .  Capati ziraryoha cyane abanyarwanda benshi bakunda capati.\",\n",
       " \"Imodoka, urupangu, ikamyo, icyuma, amatara, ikirango cy'imodoka, ibirahure.\",\n",
       " \"Inzu y'ubucuruzi icururizwamo ibinyobwa bisembuye n'ibidasembuye, umukobwa wambaye umupira uri mu ibara ry'umukara afite akamwenyu ku maso, afite igikoresho k'ikoranbuhanga kifashishwa mukwishyura ukoresheje ikarita\",\n",
       " \"Icyapa kiri munyandiko y'ururimi rw'amahanga, cyandikishijweho amagambo manini cyane ari mw'ibara ry'umweru ariko aho ari harimo ibara ry'umutuku. Hariho nimero ushobora guhamagaraho kugirango bakuyobore.\",\n",
       " \"Ahangaha turabona icyapa ni inshuti ni ko babita ariko baba bashinzwe gufata amafaranga yawe uzanye ari abanyamahanga bakaguherezamo amanyarwanda cyangwa se ukazana abanyamahanga abaye ariyo ufite bakaguhereza amanyarwanda cyangwa ukazana amanyarwanda bakaguhereza abanyamahanga bitewe n'aho ugiye kujya guhahira cyangwa icyo ugiye kuyakoresha.\",\n",
       " \"Abazungu ubona ko bafite ibintu baje  guhaha birimo imboga, inyanya, amashu, ndetse n'ibindi ipantaro, inkweto nziza, imisatsi ifunze rwose urabona ko ateruye n'indabyo arimo aragena\",\n",
       " \"Umuntu wambaye umupira w'umutuku ndetse n'itaburiya ijya gusa umukara, hirya yiwe hari undi wambaye umupira ujya gusa umweru, ndetse yambariyemo agapira kakajya gusa umukara, imbere yiwe hari amacupa abiri, hari icupa ry'amavuta y'ubuto, ndetse n'irindi cupa risa umutuku.\",\n",
       " \"Abashinzwe ubuzima mu Rwanda, bashishikariza abacuruzi bacuruza ibintu byerekanye n'amatakugira isuku; ndetse bagakorera isuku cyane ahantu batunganyiriza amata, bivugwa ko amata, iyo agiyemo umwanda ashobora gutera indwara zikabije, zirimo na korera.\"]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "references[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb4a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['akajerekani ketereze aasi kutaka kepfundi tieneza kepfundi tiju mufundiko u nyeru na akajerekani klasa na u nyeru akajerekani karimo amata',\n",
       " 'uwurijobu kweishira hakureishishge ikorana wuhanga kubi ita kubi ikuza kuhiriza ama faranga ukureishishge telefonu jendan',\n",
       " 'hano hari awa nu batatu babiri muri vo na vijitsi nagavo undi ume usigaye nu vijitsi nagori bicha iku newe zivarariji ume eru ndete imbere ya vo barikungwa amata mwira huri',\n",
       " 'aba anu benshi wa gaze imbere iinyu baku fite ya maranji yumu tuku nu mngeru ya anditseo ama gamba rimu rimu ligu icho njereza harimu ngibara li jumu tuku aba gawa warimo wa mbae imienda ita ndokanya rabamba ya mashati yu mngeru dete na mapano yumu kaina mbae ama kote na klavate hainu wa mbae ingofero na rinete',\n",
       " 'anoheza haftisukumdece na machombe wasoche rao karuhu kari amazmeza kwerikomu imetafatru ndetze na sporozi ruhura umuliza',\n",
       " 'aba anu watuara ama pichi pichi pichumunga wa heze abagode kuma pichi pichi yao vose waagaze iruhande wa hindu chie warikufu jisha awa wa heze kuma pichi pichi',\n",
       " 'awano vwenti chani wafite urvurguwela wambaye imienda itando kanyi harimo umge wichae ndetina wandi wahagaze warikule wakumeza iterezeho ibienu bitando kanyi',\n",
       " 'aba gawa wabilibicha ya hanu hamge umge ya ambaye ishati yungeru imberee harimudasobga undi ya ambaye ishati ilimamawaratandukanye nipanaroyomukara detse afitagakombe mwonochi harikongwa',\n",
       " 'mshange ya mtn ilimu uulijewa kushira mafaranga servise za mtn kwa ireza mafaranga nukunu ama faranga yohirizu wa ugandi',\n",
       " 'mfita mafranga ya maja mahanga nibijiza kukana forex impiro baka guha ammo ammajarukwa anda changu wase niifu mundwe kuhirijekuri telefoni uhanse nibijiza kukana western union baka guhu hikuuliru',\n",
       " 'mbuga wamba mwapiruri mni wararijumu karawandi tse opo risemi warariju ungeru uwaga ziku kazukami teneka mni wararijumu wadoka tanga saivise stamu kanyi nyawa kubitsa kuhereza kugura mainite ntete na kugurisha mainite mkuwako kivujokea kawa hapa rite mochangwa sehari ama karitari mni warariju ururu',\n",
       " 'kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hivyo',\n",
       " 'wenchimu waturajiwe nzifitea mufuranga ajiria wakunza kujia mwawangi changa kumilongo jitandu hii wakuresha mungama gara changu wa brand trinity wajie gufata mungu za nye kujirango bizinesi zao vizitere imbere',\n",
       " 'uvurijo uko heririza nyama faraanga atutse uheriza maanga changwa seo heririzikwa ukureshi uvurijo kama nikira',\n",
       " 'mwanyuba kujenshi za mahoteli ujendo hasanga ibikoruga vijenshi vijiebitandu kani vigufasha kuru huku aho ushovara kuasanga mwabugodjero ushovara kujamumazi ukava waruhu chiramu leze uka hasanga na hoa kuichara hile njeye kandi heza hatekani',\n",
       " 'nzui vuchuruzi idima kabati ga teletemo ibjokungo bitanuka nye amazi jriinyanje kuka nimiye mbe naza enaje halimo nichu majikusa nyamata na terimonsi njindo biteritema mandazi numu kubwa itre kunebe numu ugore itre kunebe alimokungo',\n",
       " 'ahuwa bafunjira ama faranga ya manya mahanga ushaura kuzana amadorari baka kufunjira baka guha amanya guanda ndetsewa anabusha akamadorari ufita manya guanda na mkuba kufunjira baka guha gachiro kayo',\n",
       " 'haba anuabiri umwe akawafasha telefone mwono chi wakawari kwele kana uvulijowu shoro kufashishkwa hakoeshi kwa nabuhanga hanyuma mwono akawaya kuhishula ushuzgo vitandu kanyi changosi akawasha kwele za mafuranga nuku ya achira',\n",
       " 'ndiyogo namu mwari butike harimu divinyo mga yamu kumeshi tiri mwari sirigwa aovyo aviko njishkwa kenshi binonini pia mungani',\n",
       " 'amafranga arimu kukuginoti akawariho ururi miru kujishirua dete nu mugabu menyelewe moricho jihuku akawariho mivarijendi gara gaza agachiru kayo dete nu urijo akurishku',\n",
       " 'umukawa wambaya kapila kumweli nzete endisha hati ya mawako marimari akawari kumweli mwambaye mwapila mawako marimari huwambi wakai mwabalaseke kumwela kite ufunguza mwunochi ukawa aru utumga pichi ugojitanga iguzanyo chiku biela kuwa una unguvi mzui yao kajenda uishiragache gaji',\n",
       " 'echumba jikorelgo mo uvuchuruzi gibi nhu bitanduka nye vikoresh kwa mbizima uga ugrimunzi murugo harimo ibidibuga nivikoresho vjisuku bitonde tsenesa mutu vati dosizira njeri juu mwe eru tukwabi jenewe kuburidio abaguzi wabasha kuisanzura wakarebi vjo vacheneye bagashimu',\n",
       " 'mwadamu wamba ikanzu iga njumama waratandu kanya wamba ijitambaru uicha ikunebe mbere ya kawara meza nditse atiritsa nijikombe chungeru chamata ndiyo handelgu ya mwosogu ya kawaru ndi mga wamba ishati umukara nditse ninguofiru uigururu ndiyo maya kawari cha pachanditse wahanu harama ta meza inga',\n",
       " 'umuunu fashia mafuranga munuchi mafuranga tujirumaro mkuya haisha ibiriuga ibijokambara ndetanga wakawagara gazayuko usho rakuya vunjisha aho ujana mafuranga runaka gayushaka wakawaya guha',\n",
       " 'chapatis dekza nijikwembe cha mata omono ajie kubimwa kandi arabifata na breakfasti chapatis zirajio hachani awanya guandavenshi wakunda chapati',\n",
       " 'e modoc rupangu e camiu e ciuma e matar ce rama te ai ma doc e biraguri',\n",
       " 'churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churru za churwa churwa chur',\n",
       " 'ichiapa chiri mwenyandiko yurulimiru kwa mahangu chandi chisigweho amagambo manini chane alijibu mibaradiju mngeru aliku aho alibi halimibaradiju mutuku hariko nimero shobora kwa magara ho kujirango bakuyobodi',\n",
       " 'ahangaha tulavani cha apa ninshuti nikowa vita arikowa nwa wa shingi kufata amafara anga ya uzanye ara maja mahanga baka guheriza mo amanya luguanda changu seo kaza na maja mahanga wa riyofite baka guheriza maja luguanda cheo kaza na maja luguanda baka guheriza maja mahanga bitewe na awa ujie kuja guhaira changu seo ujie kuja kuresho',\n",
       " 'uzo mkubwa nako iftivina juu kwa abidi mbingi mboga inyanya amashu tetseni mindi apanolo imaitanziza inyani misa tsefu unziru kwa sarana katerie ilinda jwa ajma rajenda',\n",
       " 'munu wambaye umupira umutuku ndetse ni taburia ijagusa umukara hirijaiwe harundi wambaye umupira ujagusungeru ndetse ya ambarie mwaga pira kajagusumukara imberiwe haramachupa abiri haruchupa jamavuta yubuto ndetse ni ndichupa risa umutuku',\n",
       " 'abashinzi kufuzi mamurgu anda bashi shikariza wa churuzi wa churuzi vijini mjerechi ya njina mata kujiri suku nitsewe kakuriri suku chane aha anu watunga njina mata bifukwa kwa amatia jimu mga anda ashubara kutiri nguara njizika vijie zilimona kuriri']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e19a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final Word Error Rate (WER)...\n",
      "\n",
      "🎉 Final Test WER: 1.4067 🎉\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 5: Calculate and Display the Final WER ---\n",
    "# ===================================================================================\n",
    "print(\"Calculating final Word Error Rate (WER)...\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "final_wer = wer_metric.compute(predictions=predictions, references=references[:32])\n",
    "\n",
    "print(f\"\\n🎉 Final Test WER: {final_wer:.4f} 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fa65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final Word Error Rate (WER)...\n",
      "\n",
      "🎉 Final Test WER: 1.4067 🎉\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 5: Calculate and Display the Final WER ---\n",
    "# ===================================================================================\n",
    "print(\"Calculating final Word Error Rate (WER)...\")\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "final_wer = wer_metric.compute(predictions=predictions, references=references[:32])\n",
    "\n",
    "print(f\"\\n🎉 Final Test WER: {final_wer:.4f} 🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbf3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to 'test_predictions.csv'...\n",
      "✅ Results saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================================\n",
    "# --- Step 6 (Optional): Save Results to a File ---\n",
    "# ===================================================================================\n",
    "print(\"Saving results to 'test_predictions.csv'...\")\n",
    "results_df = pd.DataFrame({\n",
    "    \"Reference\": references[:32],\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "results_df[\"wer\"] = results_df.apply(\n",
    "    lambda row: wer_metric.compute(predictions=[row.Prediction], references=[row.Reference]), axis=1\n",
    ")\n",
    "\n",
    "# results_df.to_csv(\"test_predictions_6k_steps__.csv\", index=False)\n",
    "print(\"✅ Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360571a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by=\"wer\", ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf0a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Akajerekani gateretse hasi ku butaka gapfundik...</td>\n",
       "      <td>akajerekani ketereze aasi kutaka kepfundi tien...</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uburyo bwo kwishyura hakoreshejwe ikoranabuhan...</td>\n",
       "      <td>uwurijobu kweishira hakureishishge ikorana wuh...</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hano hari abantu batatu babiri muri bo ni ab'i...</td>\n",
       "      <td>hano hari awa nu batatu babiri muri vo na viji...</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abantu benshi bahagaze imbere y'inyubako ifite...</td>\n",
       "      <td>aba anu benshi wa gaze imbere iinyu baku fite ...</td>\n",
       "      <td>1.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahantu heza hafite isuku ndetse n'amacumbi was...</td>\n",
       "      <td>anoheza haftisukumdece na machombe wasoche rao...</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abantu batwara amapikipiki by'umwuga bahetse a...</td>\n",
       "      <td>aba anu watuara ama pichi pichi pichumunga wa ...</td>\n",
       "      <td>1.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Abantu benshi cyane, bafite uruhu rwera, bamba...</td>\n",
       "      <td>awano vwenti chani wafite urvurguwela wambaye ...</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abagabo babiri bicaye ahantu hamwe, umwe yamba...</td>\n",
       "      <td>aba gawa wabilibicha ya hanu hamge umge ya amb...</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inshange ya emutiyeni irimo uburyo bwo kwishyu...</td>\n",
       "      <td>mshange ya mtn ilimu uulijewa kushira mafarang...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Waba ufite amafaranga y'amanyamahanga? Ni byiz...</td>\n",
       "      <td>mfita mafranga ya maja mahanga nibijiza kukana...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Umugabo wambaye umupira uri mu ibara ry'umukar...</td>\n",
       "      <td>mbuga wamba mwapiruri mni wararijumu karawandi...</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Isoko ricururizwamo ibicuruzwa bitandukanye ha...</td>\n",
       "      <td>kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hi...</td>\n",
       "      <td>4.484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Benshi mu baturage bafite amafaranga ahagije b...</td>\n",
       "      <td>wenchimu waturajiwe nzifitea mufuranga ajiria ...</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Uburyo bwo kohererezanya amafaranga aturutse w...</td>\n",
       "      <td>uvurijo uko heririza nyama faraanga atutse uhe...</td>\n",
       "      <td>1.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mu nyubako nyinshi z'amahoteli ugenda uhasanga...</td>\n",
       "      <td>mwanyuba kujenshi za mahoteli ujendo hasanga i...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Inzu y'ubucuruzi irimo akabati gateretsemo iby...</td>\n",
       "      <td>nzui vuchuruzi idima kabati ga teletemo ibjoku...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Aho bavunjira amafaranga y'amanyamahanga, usho...</td>\n",
       "      <td>ahuwa bafunjira ama faranga ya manya mahanga u...</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Abantu babiri umwe akaba afashe telefone munto...</td>\n",
       "      <td>haba anuabiri umwe akawafasha telefone mwono c...</td>\n",
       "      <td>1.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ndabona muri butike harimo ibinyobwa by'amoko ...</td>\n",
       "      <td>ndiyogo namu mwari butike harimu divinyo mga y...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Amafaranga ari mu bwoko bw'inoti akaba ariho u...</td>\n",
       "      <td>amafranga arimu kukuginoti akawariho ururi mir...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Umugabo wambaye agapira k'umweru ndetse n'isha...</td>\n",
       "      <td>umukawa wambaya kapila kumweli nzete endisha h...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Icyumba gikorerwamo ubucuruzi bw'ibintu bitand...</td>\n",
       "      <td>echumba jikorelgo mo uvuchuruzi gibi nhu bitan...</td>\n",
       "      <td>1.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Umudamu wambaye ikanzu yiganjemo amabara atand...</td>\n",
       "      <td>mwadamu wamba ikanzu iga njumama waratandu kan...</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Umuntu ufashe amafaranga mu ntoki, amafaranga ...</td>\n",
       "      <td>umuunu fashia mafuranga munuchi mafuranga tuji...</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Capati ndetse n'igikombe cy'amata umuntu agiye...</td>\n",
       "      <td>chapatis dekza nijikwembe cha mata omono ajie ...</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Imodoka, urupangu, ikamyo, icyuma, amatara, ik...</td>\n",
       "      <td>e modoc rupangu e camiu e ciuma e matar ce ram...</td>\n",
       "      <td>2.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Inzu y'ubucuruzi icururizwamo ibinyobwa bisemb...</td>\n",
       "      <td>churru za churru za churru za churru za churru...</td>\n",
       "      <td>9.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Icyapa kiri munyandiko y'ururimi rw'amahanga, ...</td>\n",
       "      <td>ichiapa chiri mwenyandiko yurulimiru kwa mahan...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ahangaha turabona icyapa ni inshuti ni ko babi...</td>\n",
       "      <td>ahangaha tulavani cha apa ninshuti nikowa vita...</td>\n",
       "      <td>1.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Abazungu ubona ko bafite ibintu baje  guhaha b...</td>\n",
       "      <td>uzo mkubwa nako iftivina juu kwa abidi mbingi ...</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Umuntu wambaye umupira w'umutuku ndetse n'itab...</td>\n",
       "      <td>munu wambaye umupira umutuku ndetse ni taburia...</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Abashinzwe ubuzima mu Rwanda, bashishikariza a...</td>\n",
       "      <td>abashinzi kufuzi mamurgu anda bashi shikariza ...</td>\n",
       "      <td>1.258065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Reference  \\\n",
       "0   Akajerekani gateretse hasi ku butaka gapfundik...   \n",
       "1   Uburyo bwo kwishyura hakoreshejwe ikoranabuhan...   \n",
       "2   Hano hari abantu batatu babiri muri bo ni ab'i...   \n",
       "3   Abantu benshi bahagaze imbere y'inyubako ifite...   \n",
       "4   Ahantu heza hafite isuku ndetse n'amacumbi was...   \n",
       "5   Abantu batwara amapikipiki by'umwuga bahetse a...   \n",
       "6   Abantu benshi cyane, bafite uruhu rwera, bamba...   \n",
       "7   Abagabo babiri bicaye ahantu hamwe, umwe yamba...   \n",
       "8   Inshange ya emutiyeni irimo uburyo bwo kwishyu...   \n",
       "9   Waba ufite amafaranga y'amanyamahanga? Ni byiz...   \n",
       "10  Umugabo wambaye umupira uri mu ibara ry'umukar...   \n",
       "11  Isoko ricururizwamo ibicuruzwa bitandukanye ha...   \n",
       "12  Benshi mu baturage bafite amafaranga ahagije b...   \n",
       "13  Uburyo bwo kohererezanya amafaranga aturutse w...   \n",
       "14  Mu nyubako nyinshi z'amahoteli ugenda uhasanga...   \n",
       "15  Inzu y'ubucuruzi irimo akabati gateretsemo iby...   \n",
       "16  Aho bavunjira amafaranga y'amanyamahanga, usho...   \n",
       "17  Abantu babiri umwe akaba afashe telefone munto...   \n",
       "18  Ndabona muri butike harimo ibinyobwa by'amoko ...   \n",
       "19  Amafaranga ari mu bwoko bw'inoti akaba ariho u...   \n",
       "20  Umugabo wambaye agapira k'umweru ndetse n'isha...   \n",
       "21  Icyumba gikorerwamo ubucuruzi bw'ibintu bitand...   \n",
       "22  Umudamu wambaye ikanzu yiganjemo amabara atand...   \n",
       "23  Umuntu ufashe amafaranga mu ntoki, amafaranga ...   \n",
       "24  Capati ndetse n'igikombe cy'amata umuntu agiye...   \n",
       "25  Imodoka, urupangu, ikamyo, icyuma, amatara, ik...   \n",
       "26  Inzu y'ubucuruzi icururizwamo ibinyobwa bisemb...   \n",
       "27  Icyapa kiri munyandiko y'ururimi rw'amahanga, ...   \n",
       "28  Ahangaha turabona icyapa ni inshuti ni ko babi...   \n",
       "29  Abazungu ubona ko bafite ibintu baje  guhaha b...   \n",
       "30  Umuntu wambaye umupira w'umutuku ndetse n'itab...   \n",
       "31  Abashinzwe ubuzima mu Rwanda, bashishikariza a...   \n",
       "\n",
       "                                           Prediction       wer  \n",
       "0   akajerekani ketereze aasi kutaka kepfundi tien...  1.125000  \n",
       "1   uwurijobu kweishira hakureishishge ikorana wuh...  1.250000  \n",
       "2   hano hari awa nu batatu babiri muri vo na viji...  0.793103  \n",
       "3   aba anu benshi wa gaze imbere iinyu baku fite ...  1.292683  \n",
       "4   anoheza haftisukumdece na machombe wasoche rao...  0.944444  \n",
       "5   aba anu watuara ama pichi pichi pichumunga wa ...  1.421053  \n",
       "6   awano vwenti chani wafite urvurguwela wambaye ...  0.954545  \n",
       "7   aba gawa wabilibicha ya hanu hamge umge ya amb...  0.964286  \n",
       "8   mshange ya mtn ilimu uulijewa kushira mafarang...  1.000000  \n",
       "9   mfita mafranga ya maja mahanga nibijiza kukana...  1.000000  \n",
       "10  mbuga wamba mwapiruri mni wararijumu karawandi...  0.957447  \n",
       "11  kwa hivyo kwa hivyo kwa hivyo kwa hivyo kwa hi...  4.484848  \n",
       "12  wenchimu waturajiwe nzifitea mufuranga ajiria ...  0.966667  \n",
       "13  uvurijo uko heririza nyama faraanga atutse uhe...  1.071429  \n",
       "14  mwanyuba kujenshi za mahoteli ujendo hasanga i...  1.000000  \n",
       "15  nzui vuchuruzi idima kabati ga teletemo ibjoku...  1.000000  \n",
       "16  ahuwa bafunjira ama faranga ya manya mahanga u...  1.333333  \n",
       "17  haba anuabiri umwe akawafasha telefone mwono c...  1.035714  \n",
       "18  ndiyogo namu mwari butike harimu divinyo mga y...  1.000000  \n",
       "19  amafranga arimu kukuginoti akawariho ururi mir...  1.000000  \n",
       "20  umukawa wambaya kapila kumweli nzete endisha h...  1.000000  \n",
       "21  echumba jikorelgo mo uvuchuruzi gibi nhu bitan...  1.029412  \n",
       "22  mwadamu wamba ikanzu iga njumama waratandu kan...  0.953488  \n",
       "23  umuunu fashia mafuranga munuchi mafuranga tuji...  0.962963  \n",
       "24  chapatis dekza nijikwembe cha mata omono ajie ...  0.950000  \n",
       "25  e modoc rupangu e camiu e ciuma e matar ce ram...  2.125000  \n",
       "26  churru za churru za churru za churru za churru...  9.208333  \n",
       "27  ichiapa chiri mwenyandiko yurulimiru kwa mahan...  1.000000  \n",
       "28  ahangaha tulavani cha apa ninshuti nikowa vita...  1.285714  \n",
       "29  uzo mkubwa nako iftivina juu kwa abidi mbingi ...  1.040000  \n",
       "30  munu wambaye umupira umutuku ndetse ni taburia...  0.789474  \n",
       "31  abashinzi kufuzi mamurgu anda bashi shikariza ...  1.258065  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
